<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sisyphus&#39;s Utopia</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-24T18:39:13.120Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Twist Nihility</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>JavaScript： 先天不足的畸形儿</title>
    <link href="http://yoursite.com/2018/04/25/the-history-of-javascript/"/>
    <id>http://yoursite.com/2018/04/25/the-history-of-javascript/</id>
    <published>2018-04-24T17:38:13.000Z</published>
    <updated>2018-04-24T18:39:13.120Z</updated>
    
    <content type="html"><![CDATA[<p>前端是痛苦的。先来看这几个名词：ECMAScript、JavaScript、Java、JScript。<br>曾经，有同学问我：“JavaScript和Java是什么关系？”我说，就是社会主义和中国特色社会主义之间的关系。<br>先来解释上面的这几个名词吧。当年网景公司开发出了一门浏览器语言，为了蹭Java的热度，和Sun公司一起搞了个大新闻，把这门浏览器脚本语言命名为JavaScript——人家就是故意让大家误会的。JavaScript后来上交给了国际标准化组织ECMA。ECMA在第一版的JavaScript的基础上制定了一个标准，这就是ECMAScript。总的来说，ECMAScript是实现标准，JavaScript和JScript以及后来诞生的许多XXScript脚本语言都是ECMAScript的实现。ECMAScript是普通话/汉语/中文，JavaScript是北京话，JScript是粤语。<br>除了名称上的混乱，前端开发的一大困境就是要面对数不清的框架。Angular.js，jQuery，Node.js，element.js，vue.js等等等等，一年出一个新框架，一辈子都学不完。。每一个从其他语言迁移来的都很疑惑，到底是为什么？为什么JavaScript会诞生这么多的变体？<br>我想，最好的办法就是回溯JavaScript出生的那一天，回到它的产房，看看那一天究竟发生了什么。<br>JavaScript诞生于浏览器的鼻祖网景公司（Netscape）。大约是1994年左右，网景公司（Netscape）发布了Navigator浏览器0.9版，这是一款很经典的浏览器，网景公司（Netscape）的用户数因此而出现了井喷的态势，但是Navigator0.9不具备和访问者互动的能力，在那个上网速度比蜗牛还慢的时代，网景公司（Netscape）急需一种脚本语言，使得浏览器和网页进行交互，从而提升用户的体验。<br>针对这个问题，网景公司（Netscape）有两种选择，一是采用现有的脚本语言，二是自己发明一个新的脚本语言。当时网景公司（Netscape）的高层对这个问题争论不休。在这些喋喋不休的争论里，时间走到了1995年。这一年发生了一件创造历史的大事件：编程语言Java横空出世，Java凭借“一次编写，到处运行的”强大宣传，大有未来主宰的霸气，这些让网景公司（Netscape）高层们一下子被Java所俘获，如是网景公司（Netscape）和Sun公司结盟，网景公司（Netscape）不仅允许Java程序以applet的形式嵌入到浏览器，直接在浏览器里面运行，甚至还打算把Java作为脚本嵌入到网页，只是最后发现网页会变的过于复杂而放弃，但是JavaScript的Java印记永远都挥之不去。<br>事情的转折发生在1995年4月，网景公司（Netscape）录用了Bremdan Eich(布兰登·艾奇)（虽然Bremdan Eich(布兰登·艾奇)是JavaScript的祖师爷，但是他的介入或许也是JavaScript悲剧的开始）。我们还是接着说网景公司（Netscape）吧。1995年5月，网景公司（Netscape）做出了决策，未来的网页脚本语言必须看上去和Java足够相似，但是比Java简单，使得非专业的网页作者能很快的上手。<br>Bremdan Eich(布兰登·艾奇)被任命为这个简化版的Java的设计师。但是Bremdan Eich(布兰登·艾奇)对Java一点兴趣都没有，为了应付公司的安排的任务，他只用10天时间就设计出了JavaScript。悲剧就这么诞生了。<br>Brendan Eich的主要方向和兴趣是函数式编程，网景公司招聘他的目的，是研究将Scheme语言作为网页脚本语言的可能性。Brendan Eich本人也是这样想的，以为进入新公司后，会主要与Scheme语言打交道。<br>10天诞生一种语言，不管怎么说还是要把Brendan Eich当神看。但是这位神创造世界的时间实在太短了，导致很多细节考虑不周，因此JavaScript写出的程序混乱不堪，成了许多程序员的梦魇，差点被人抛弃，直到ajax的出世，才让人们终于找到理由忍受它的畸形。<br>总的来说啊，Brendan Eich设计思路是这样的：</p><ul><li>借鉴C语言的基本语法；</li></ul><ul><li>借鉴Java语言的数据类型和内存管理；</li></ul><ul><li>借鉴Scheme语言，将函数提升到”第一等公民”（first class）的地位；</li></ul><ul><li>借鉴Self语言，使用基于原型（prototype）的继承机制。<br>所以，JavaScript语言实际上是两种语言风格的混合产物（简化的）函数式编程+（简化的）面向对象编程。这是由Brendan Eich（函数式编程）与网景公司（面向对象编程）共同决定的。<br>不管怎么说，JavaScript和Java是有关系的，JavaScript里面有Java的思想。所以说JavaScript和Java无关是不正确的。<br>其实一直到现在Brendan Eich还是看不起讨厌Java。假如不是公司决策Brendan Eich绝对不会把Java作为JavaScript的设计原型，即使是现在，Brendan Eich还是讨厌自己的作品。他曾经说过：“与其说我爱JavaScript，不如说我恨它。它是C语言和Self语言的产物。十八世纪英国文学家约翰逊博士说得好：’它的优秀之处并非原创，它的原创之处并不优秀。”是不是很像不受父母欢迎的私生子？<br>所有人第一次接触JavaScript面向对象编程时候，大概都是忍着刺痛和模糊看完的。猎奇也好，误解也罢，很多人觉得JavaScript面向对象编程是代码爱好者的游戏，使用价值不大，但其实，JavaScript面向对象编程用途是如此之多令我叹为观止。我们可以说：最好的JavaScript代码都应该是面向对象的。<br>那么JavaScript里是如何实现继承的？JavaScript的继承机制如何？<br>首先JavaScript里面没有”子类”和”父类”的概念，也没有”类”（class）和”实例”（instance）的区分，全靠一种很奇特的”原型链”（prototype chain）模式，来实现继承。<br>网景公司在发明与设计JavaScript的目标，其中很重要的两点：<br>1.简易版的Java；<br>2.简易，简易还是简易。<br>Brendan Eich设计JavaScript的时候引入了Java一个非常重要的概念：一切皆对象。既然JavaScript里面有了对象，那么设不设计继承就是困扰Brendan Eich的一个问题，如果真是要设计一个简易的语言其实可以不要继承机制，继承属于专业的程序员，但是JavaScript里那么多的对象，如果没有一种机制，他们之间将如何联系了，这必然会对编写程序的可靠性带来很大的问题，但是引入了继承又会使用JavaScript变成了完整的面向对象的语言，从而提高了它的门槛，让很多初学者望而却步，折中之下，Brendan Eich还是选择设计继承，但绝不是标准的继承（说道这里不禁让人想起同样使用EMCAScript标准设计的语言ActionScript，它里面就有很完整的继承，做起来很惬意，不知道这是不是JavaScript以后的趋势，说不定哪天JavaScript会变的更完美了？）。折中是指Brendan Eich不打算引入类（class），这样JavaScript至少看起来不像面向对象的语言了，那么初学者就不会望而却步了（这是赤裸裸的欺骗，进来后倒腾死你，关门打狗，莫过如此）。<br>Brendan Eich思考之后，决定借鉴C++和java的new命令，将new命令引入了JavaScript，在传统的面向对象的语言里，new 用来构造实例对象，new 会调用构造函数，但是传统面向对象的语言new后面的是类，内部机制是调用构造函数（constructor），而Brendan Eich简化了这个操作，在JavaScript里面，new 后面直接是构造函数，如是我们可以这么写一个Person类：<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">Person</span>(<span class="params">name</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"> <span class="keyword">this</span>.name = name;</span><br><span class="line">&#125;</span><br><span class="line">  </span><br><span class="line"><span class="keyword">var</span> per = <span class="keyword">new</span> Person(<span class="string">'Brendan Eich'</span>);</span><br><span class="line"><span class="built_in">console</span>.log(per.name);<span class="comment">//Brendan Eich</span></span><br></pre></td></tr></table></figure></li></ul><p>这样就创建了一个新的实例了。但是new有缺陷。用构造函数生成实例对象是无法无法共享属性和方法，例如下面代码：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">Person</span>(<span class="params">name</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="keyword">this</span>.name = name;</span><br><span class="line">     <span class="keyword">this</span>.nation = <span class="string">'USA'</span>;</span><br><span class="line">&#125;</span><br><span class="line">  </span><br><span class="line"><span class="keyword">var</span> per1 = <span class="keyword">new</span> Person(<span class="string">'Brendan Eich'</span>);</span><br><span class="line"><span class="keyword">var</span> per2 = <span class="keyword">new</span> Person(<span class="string">'蒟蒻'</span>);</span><br><span class="line">per2.nation = <span class="string">'China'</span>;</span><br><span class="line"><span class="built_in">console</span>.log(per1.nation);<span class="comment">//USA</span></span><br><span class="line"><span class="built_in">console</span>.log(per2.nation);<span class="comment">//China</span></span><br></pre></td></tr></table></figure></p><p>每一个实例对象，都有自己的属性和方法的副本。这不仅无法做到数据共享，也是极大的资源浪费。和JavaScript工厂模式的缺点一样，过多重复的对象会使得浏览器速度缓慢，造成资源的极大的浪费。<br>考虑到这一点，Brendan Eich决定为构造函数设置一个prototype属性，这个属性都是指向一个prototype对象。下面一句话很重要：所有实例对象需要共享的属性和方法，都放在这个对象里面；那些不需要共享的属性和方法，就放在构造函数里面。<br>实例对象一旦创建，将自动引用prototype对象的属性和方法。也就是说，实例对象的属性和方法，分成两种，一种是本地的，另一种是引用的。如是我们可以改写下上面的程序：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">Person</span>(<span class="params">name</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"> <span class="keyword">this</span>.name = name;</span><br><span class="line">&#125; </span><br><span class="line">Person.prototype = &#123;<span class="attr">nation</span>:<span class="string">'USA'</span>&#125;;</span><br><span class="line"><span class="keyword">var</span> per1 = <span class="keyword">new</span> Person(<span class="string">'Brendan Eich'</span>);</span><br><span class="line"><span class="keyword">var</span> per2 = <span class="keyword">new</span> Person(<span class="string">'IT民工'</span>); </span><br><span class="line"><span class="built_in">console</span>.log(per1.nation);<span class="comment">//USA</span></span><br><span class="line"><span class="built_in">console</span>.log(per2.nation);<span class="comment">//USA</span></span><br></pre></td></tr></table></figure></p><p>当我们这样写程序时候Person.prototype.nation = ‘China’; 所有实例化的类的nation都会变成China。<br>由于所有的实例对象共享同一个prototype对象，那么从外界看起来，prototype对象就好像是实例对象的原型，而实例对象则好像”继承”了prototype对象一样。prototype只是提供了实现JavaScript继承的一个很方便的途径和手段。<br>原型链是学习JavaScript最大的难点之一。JavaScript足够独特，一路走来足够坎坷。我觉得，当疑惑于JavaScript的某个特性时，不妨回溯历史，仔细体会一下人类智慧的光辉痕迹，也许会有豁然开朗之感。<br>JavaScript，网景，以及之后著名的“浏览器大战”，是我本人很喜欢的一段科技史。这段历史，带有一种天生的厚重感。那个时候，波澜壮阔的互联网时代即将拉开帷幕，一片崭新的天地之下，所有事物尚需指指点点。<br>作为一门先天不足的语言，JavaScript一路走来经历了太多。有人爱它也好，有人恨它也罢，它兀自默默发着自己的光。我偏爱这样的故事。带着罪恶，残缺和不足诞生，甚至亲生的父母都嫌弃，迎接漫天的谩骂和否定，却仍然带着希望迎来广阔的未来。<br>很燃很热血。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前端是痛苦的。先来看这几个名词：ECMAScript、JavaScript、Java、JScript。&lt;br&gt;曾经，有同学问我：“JavaScript和Java是什么关系？”我说，就是社会主义和中国特色社会主义之间的关系。&lt;br&gt;先来解释上面的这几个名词吧。当年网景公司开发
      
    
    </summary>
    
      <category term="科普趣谈" scheme="http://yoursite.com/categories/%E7%A7%91%E6%99%AE%E8%B6%A3%E8%B0%88/"/>
    
    
      <category term="JavaScript" scheme="http://yoursite.com/tags/JavaScript/"/>
    
  </entry>
  
  <entry>
    <title>TFMLC学习笔记（2） TensorFlow进阶</title>
    <link href="http://yoursite.com/2018/04/22/TFMLC-2/"/>
    <id>http://yoursite.com/2018/04/22/TFMLC-2/</id>
    <published>2018-04-22T10:50:03.000Z</published>
    <updated>2018-04-23T14:34:01.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算图中的操作"><a href="#计算图中的操作" class="headerlink" title="计算图中的操作"></a>计算图中的操作</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入TensorFlow，创建一个会话，开始一个计算图</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess=tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment">#传入一个列表到计算图中的操作，并打印返回值</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x_vals=np.array([<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">5.</span>, <span class="number">7.</span>, <span class="number">9.</span>])                            <span class="comment">#创建一个numpy数组</span></span><br><span class="line">x_data=tf.palceholder(tf.float32)                                <span class="comment">#声明占位符</span></span><br><span class="line">m_const=tf.constant(<span class="number">3.</span>)                                          <span class="comment">#声明张量</span></span><br><span class="line">my_product=tf.mul(x_data,m_const)</span><br><span class="line"><span class="keyword">for</span> x_val <span class="keyword">in</span> x_vals:</span><br><span class="line">    print(sess.run(my_product,feed_dict=&#123;x_data:x_val&#125;))</span><br><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">9.0</span></span><br><span class="line"><span class="number">15.0</span></span><br><span class="line"><span class="number">21.0</span></span><br><span class="line"><span class="number">27.0</span></span><br></pre></td></tr></table></figure><h1 id="TensorFlow的嵌入Layer"><a href="#TensorFlow的嵌入Layer" class="headerlink" title="TensorFlow的嵌入Layer"></a>TensorFlow的嵌入Layer</h1><p>本节学习如何在同一个计算图中进行多个乘法操作。下面将用两个矩阵乘以占位符，然后做加法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入TensorFlow，创建一个会话，开始一个计算图</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess=tf.Session()</span><br></pre></td></tr></table></figure></p><p>我们将传入两个形状为3x5的numpy数组，然后每个矩阵乘以常量矩阵（形状：5x1），将返回一个形状为3x1的矩阵。然后再乘以1x1的矩阵，返回的结果矩阵仍然为3x1。最后，加上一个3x1的矩阵。</p><ol><li><p>首先，创建数据和占位符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_array=np.array([[ <span class="number">1.</span>, <span class="number">3.</span>,<span class="number">5.</span>,<span class="number">7.</span>,<span class="number">9.</span>],</span><br><span class="line">                   [<span class="number">-2.</span>, <span class="number">0.</span>,<span class="number">2.</span>,<span class="number">4.</span>,<span class="number">6.</span>],</span><br><span class="line">                   [<span class="number">-6.</span>,<span class="number">-3.</span>,<span class="number">0.</span>,<span class="number">3.</span>,<span class="number">6.</span>]])</span><br><span class="line">x_vals=np.array([my_array,my_array+<span class="number">1</span>])</span><br><span class="line">x_data=tf.placeholder(tf.float32,shape=(<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure></li><li><p>接着，创建矩阵乘法和加法中要用到的常量矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m1=tf.constant([[<span class="number">1.</span>],[<span class="number">0.</span>],[<span class="number">-1.</span>],[<span class="number">2.</span>],[<span class="number">4.</span>]])</span><br><span class="line">m2=tf.constant([[<span class="number">2.</span>]])</span><br><span class="line">a1=tf.constant([[<span class="number">10.</span>]])</span><br></pre></td></tr></table></figure></li><li><p>声明操作，表示成计算图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prod1=tf.matmul(x_data,m1)</span><br><span class="line">prod2=tf.matmul(prod1,m2)</span><br><span class="line">add1=tf.add(prod2,a1)</span><br></pre></td></tr></table></figure></li><li><p>最后，通过计算图赋值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x_val <span class="keyword">in</span> x_vals:</span><br><span class="line">    print(sess.run(add1,feed_dict=&#123;x_data:x_val&#125;))</span><br><span class="line">[[<span class="number">102.</span>]</span><br><span class="line"> [ <span class="number">66.</span>]</span><br><span class="line"> [ <span class="number">58.</span>]]</span><br><span class="line">[[<span class="number">114.</span>]</span><br><span class="line"> [ <span class="number">78.</span>]</span><br><span class="line"> [ <span class="number">70.</span>]]</span><br></pre></td></tr></table></figure></li></ol><h1 id="TensorFlow的多层Layer"><a href="#TensorFlow的多层Layer" class="headerlink" title="TensorFlow的多层Layer"></a>TensorFlow的多层Layer</h1><p>本节中，将介绍如何更好地连接多层Layer，包括自定义Layer。这里给出一个例子（数据是生成随机图片数据），以更好地理解不同类型的操作和如何利用内建层Layer进行计算。我们对2D图像进行滑动窗口平均，然后通过自定义操作层Layer返回结果。<br>在本节，TensorFlow的计算图太大，导致无法完整查看。为了解决此问题，将对各层Layer和操作进行层级命名管理。<br>按照惯例，加载numpy和TensorFlow模块，创建计算图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">sess=tf.Session()</span><br></pre></td></tr></table></figure></p><ol><li><p>首先通过numpy创建2D图像，4x4像素图片。我们将创建成四维：第一维和最后一维大小为1。注意TensorFlow的图像函数是处理四维图片的，这四维是：图片数量、高度、宽度、颜色通道。这里是一张图片，单颜色通道，所以设两个维度值为1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_shape=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line">x_val=np.random.uniform(size=x_shape)</span><br></pre></td></tr></table></figure></li><li><p>下面在计算图中创建占位符。此例中占位符是用来传入图片的，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_data=tf.placeholder(tf.float32,shape=x_shape)</span><br></pre></td></tr></table></figure></li><li><p>为了创建过滤4x4像素图片的滑动窗口，我们将用TensorFlow内建函数conv2d()（常用来做图像处理）卷积2x2形状的常量窗口。con2d()函数传入滑动窗口、过滤器和步长。本例将在滑动窗口四个方向上计算，所以在四个方向上都要指定步长。创建一个2x2的窗口，每个方向长度为2的步长。为了计算平均值，用常量为0.25的向量与2x2的窗口卷积：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_filter=tf.constant(<span class="number">0.25</span>,shape=[<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">my_strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>]</span><br><span class="line">mov_avg_layer=tf.nn.conv2d(x_data,my_filter,my_strides,padding=<span class="string">'SAME'</span>,name=<span class="string">'Moving_Avg_Window'</span>)</span><br></pre></td></tr></table></figure></li><li><p>注意，我们通过conv2d()函数的name参数，把这层Layer命名为“Moving_Avg_Window”。</p></li><li><p>现在定义一个自定义Layer，操作滑动窗口平均的2x2的返回值。自定义函数将输入张量乘以一个2x2的矩阵张量，然后每个元素加1.因为矩阵乘法只计算二维矩阵，所以剪裁图像的多余维度（大小为1）。TensorFlow通过内建函数squeeze()剪裁。下面是新定义的Layer：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a custom layer which will be sigmoid(Ax+b) where x is a 2x2 matrix and A and b are 2x2 matrices</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_layer</span><span class="params">(input_matrix)</span>:</span></span><br><span class="line">    input_matrix_sqeezed = tf.squeeze(input_matrix)</span><br><span class="line">    A = tf.constant([[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">-1.</span>, <span class="number">3.</span>]])</span><br><span class="line">    b = tf.constant(<span class="number">1.</span>, shape=[<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">    temp1 = tf.matmul(A, input_matrix_sqeezed)</span><br><span class="line">    temp = tf.add(temp1, b) <span class="comment"># Ax + b</span></span><br><span class="line">    <span class="keyword">return</span>(tf.sigmoid(temp))</span><br></pre></td></tr></table></figure></li><li><p>现在把刚刚新定义的Layer加入到计算图中，并且用tf.name_scope()命名唯一的Layer名字，后续在计算图中可折叠/扩展Custom_Layer层。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add custom layer to graph</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Custom_Layer'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    custom_layer1 = custom_layer(mov_avg_layer)</span><br></pre></td></tr></table></figure></li><li><p>为占位符传入4x4像素图片，然后执行计算图，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># After custom operation, size is now 2x2 (squeezed out size 1 dims)</span></span><br><span class="line">print(sess.run(custom_layer1, feed_dict=&#123;x_data: x_val&#125;))</span><br><span class="line">[[<span class="number">0.91914582</span> <span class="number">0.96025133</span>]</span><br><span class="line"> [<span class="number">0.87262219</span> <span class="number">0.9469803</span>]]</span><br></pre></td></tr></table></figure></li></ol><h1 id="TensorFlow实现损失函数"><a href="#TensorFlow实现损失函数" class="headerlink" title="TensorFlow实现损失函数"></a>TensorFlow实现损失函数</h1><p>为了优化机器学习学习算法，我们需要评估机器学习模型训练输出结果。在TensorFlow中评估输出结果依赖损失函数。损失函数告诉TensorFlow，预测结果相比期望的结果是好是坏。在大部分场景下，我们会有算法模型训练的样本数据集和目标值。损失函数比较预测值与目标值，并给出两者之间的数值化的差值。<br>为了比较不同损失函数的区别，需要用图表将它们绘制出来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt      <span class="comment">#加载matplotlib（Python的绘图库）</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure></p><ol><li>回归算法的损失函数。回归算法是预测连续因变量的。创建预测序列和目标序列作为张量，预测序列是-1到1之间的等差数列。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Numerical Predictions</span></span><br><span class="line">x_vals = tf.linspace(<span class="number">-1.</span>, <span class="number">1.</span>, <span class="number">500</span>)</span><br><span class="line">target = tf.constant(<span class="number">0.</span>)</span><br></pre></td></tr></table></figure></li></ol><ul><li>L2正则损失函数（欧拉损失函数）<br>L2正则损失函数是预测值与目标值差值的平方和。注意：上述例子中目标值为0。L2正则损失函数是非常有用的损失函数，因为它在目标值附近有更好的曲度，机器学习算法利用这点收敛，并且离目标越近收敛越慢。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L2 loss</span></span><br><span class="line"><span class="comment"># L = (pred - actual)^2</span></span><br><span class="line">l2_y_vals=tf.square(target-x_vals)</span><br><span class="line">l2_y_out=sess.run(l2_y_vals)</span><br></pre></td></tr></table></figure></li></ul><p>TensorFlow有内建的L2正则形式，称为nn.l2_loss()。这个函数是实际L2正则的一半，即，上述l2_y_vals的1/2</p><ul><li>L1正则损失函数（绝对值损失函数）<br>与L2正则损失函数对差值求平方不同的是，L1正则损失函数对差值求绝对值。L1正则在目标值附近不平滑，这会导致算法不能很好的收敛。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L1 loss</span></span><br><span class="line"><span class="comment"># L = abs(pred - actual)</span></span><br><span class="line">l1_y_vals = tf.abs(target - x_vals)</span><br><span class="line">l1_y_out = sess.run(l1_y_vals)</span><br></pre></td></tr></table></figure></li></ul><ul><li>Pseudo-Huber损失函数<br>Pseudo-Huber损失函数是Huber损失函数的连续、平滑估计，试图利用L1和L2正则削减极值处的陡峭，使得目标值附近连续。它的表达式依赖参数delta。我们将绘图来显示delta1=0.25和delta2=5的区别：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pseudo-Huber loss</span></span><br><span class="line"><span class="comment"># L = delta^2 * (sqrt(1 + ((pred - actual)/delta)^2) - 1)</span></span><br><span class="line">delta1 = tf.constant(<span class="number">0.25</span>)</span><br><span class="line">phuber1_y_vals = tf.multiply(tf.square(delta1), tf.sqrt(<span class="number">1.</span> + tf.square((target - x_vals)/delta1)) - <span class="number">1.</span>)</span><br><span class="line">phuber1_y_out = sess.run(phuber1_y_vals)</span><br><span class="line"></span><br><span class="line">delta2 = tf.constant(<span class="number">5.</span>)</span><br><span class="line">phuber2_y_vals = tf.multiply(tf.square(delta2), tf.sqrt(<span class="number">1.</span> + tf.square((target - x_vals)/delta2)) - <span class="number">1.</span>)</span><br><span class="line">phuber2_y_out = sess.run(phuber2_y_vals)</span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li>分类损失函数<br>分类损失函数用来评估预测分类结果。<br>重新给x_vals和target赋值，保存返回值并在下节绘制出来。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_vals = tf.linspace(<span class="number">-3.</span>, <span class="number">5.</span>, <span class="number">500</span>)</span><br><span class="line">target = tf.constant(<span class="number">1.</span>)</span><br><span class="line">targets = tf.fill([<span class="number">500</span>,], <span class="number">1.</span>)</span><br></pre></td></tr></table></figure></li></ol><ul><li>Hinge损失函数<br>Hinge损失函数主要用来评估支持向量机算法，但有时也用来评估神经网络算法。在本例中是计算两个目标类（-1,1）之间的损失。下面的代码中，使用目标值1，所以预测值离1越近，损失函数值越小：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hinge loss</span></span><br><span class="line"><span class="comment"># Use for predicting binary (-1, 1) classes</span></span><br><span class="line"><span class="comment"># L = max(0, 1 - (pred * actual))</span></span><br><span class="line">hinge_y_vals = tf.maximum(<span class="number">0.</span>, <span class="number">1.</span> - tf.multiply(target, x_vals))</span><br><span class="line">hinge_y_out = sess.run(hinge_y_vals)</span><br></pre></td></tr></table></figure></li></ul><ul><li>两类交叉熵损失函数（Cross-entropy loss）<br>两类交叉熵损失函数（Cross-entropy loss）有时也作为逻辑损失函数。比如，当预测两类目标0或者1时，希望度量预测值到真实分类值（0或1）的距离，这个距离经常是0到1之间的实数。为了度量这个距离，使用信息论中的交叉熵。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cross entropy loss</span></span><br><span class="line"><span class="comment"># L = -actual * (log(pred)) - (1-actual)(log(1-pred))</span></span><br><span class="line">xentropy_y_vals = - tf.multiply(target, tf.log(x_vals)) - tf.multiply((<span class="number">1.</span> - target), tf.log(<span class="number">1.</span> - x_vals))</span><br><span class="line">xentropy_y_out = sess.run(xentropy_y_vals)</span><br></pre></td></tr></table></figure></li></ul><ul><li>Sigmoid交叉熵损失函数（Sigmoid cross entropy loss）<br>Sigmoid交叉熵损失函数与两类交叉熵损失函数非常相似，区别在于：Sigmoid交叉熵损失函数先把x_vals值通过sigmoid函数转换，再计算交叉熵损失。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xentropy_sigmoid_y_vals=tf.nn.sigmoid_cross_entropy_with_logits(x_vals,targets)</span><br><span class="line">xentropy_sigmoid_y_out=sess.run(xentropy_sigmoid_y_vals)</span><br></pre></td></tr></table></figure></li></ul><ul><li>加权交叉熵损失函数（Weighted cross entropy loss）<br>加权交叉熵损失函数是Sigmoid交叉熵损失函数的加权，对正目标加权。例如，将正目标加权权重0.5<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Weighted (softmax) cross entropy loss</span></span><br><span class="line"><span class="comment"># L = -actual * (log(pred)) * weights - (1-actual)(log(1-pred))</span></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="comment"># L = (1 - pred) * actual + (1 + (weights - 1) * pred) * log(1 + exp(-actual))</span></span><br><span class="line">weight = tf.constant(<span class="number">0.5</span>)</span><br><span class="line">xentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(x_vals,targets,weight)</span><br><span class="line">xentropy_weighted_y_out = sess.run(xentropy_weighted_y_vals)</span><br></pre></td></tr></table></figure></li></ul><ul><li>Softmax交叉熵损失函数（Softmax cross-entropy loss）<br>Softmax交叉熵损失函数是作用于非归一化的输出结果，只针对单个目标分类的计算损失。通过softmax函数将输出结果转化成概率分布，然后计算真值概率分布的损失<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Softmax entropy loss</span></span><br><span class="line"><span class="comment"># L = -actual * (log(softmax(pred))) - (1-actual)(log(1-softmax(pred)))</span></span><br><span class="line">unscaled_logits = tf.constant([[<span class="number">1.</span>, <span class="number">-3.</span>, <span class="number">10.</span>]])</span><br><span class="line">target_dist = tf.constant([[<span class="number">0.1</span>, <span class="number">0.02</span>, <span class="number">0.88</span>]])</span><br><span class="line">softmax_xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(unscaled_logits,target_dist)</span><br><span class="line">print(sess.run(softmax_xentropy))</span><br><span class="line">[<span class="number">1.16012561</span>]</span><br></pre></td></tr></table></figure></li></ul><ul><li>稀疏Softmax交叉熵损失函数（Sparse softmax cross-entropy loss）<br>稀疏Softmax交叉熵损失函数和Softmax交叉熵损失函数类似，它是把目标分类为true的转化成index，而Softmax交叉熵损失函数将目标转成概率分布。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sparse entropy loss</span></span><br><span class="line"><span class="comment"># Use when classes and targets have to be mutually exclusive</span></span><br><span class="line"><span class="comment"># L = sum( -actual * log(pred) )</span></span><br><span class="line">unscaled_logits = tf.constant([[<span class="number">1.</span>, <span class="number">-3.</span>, <span class="number">10.</span>]])</span><br><span class="line">sparse_target_dist = tf.constant([<span class="number">2</span>])</span><br><span class="line">sparse_xentropy =  tf.nn.sparse_softmax_cross_entropy_with_logits(unscaled_logits,sparse_target_dist)</span><br><span class="line">print(sess.run(sparse_xentropy))</span><br><span class="line">[<span class="number">0.00012564</span>]</span><br></pre></td></tr></table></figure></li></ul><p>3.绘制各类损失函数</p><ul><li>用matplotlib绘制回归算法的损失函数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the output:</span></span><br><span class="line">x_array = sess.run(x_vals)</span><br><span class="line">plt.plot(x_array, l2_y_out, <span class="string">'b-'</span>, label=<span class="string">'L2 Loss'</span>)</span><br><span class="line">plt.plot(x_array, l1_y_out, <span class="string">'r--'</span>, label=<span class="string">'L1 Loss'</span>)</span><br><span class="line">plt.plot(x_array, phuber1_y_out, <span class="string">'k-.'</span>, label=<span class="string">'P-Huber Loss (0.25)'</span>)</span><br><span class="line">plt.plot(x_array, phuber2_y_out, <span class="string">'g:'</span>, label=<span class="string">'P-Huber Loss (5.0)'</span>)</span><br><span class="line">plt.ylim(<span class="number">-0.2</span>, <span class="number">0.4</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>, prop=&#123;<span class="string">'size'</span>: <span class="number">11</span>&#125;)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li></ul><ul><li>用matplotlib绘制分类算法的损失函数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the output</span></span><br><span class="line">x_array = sess.run(x_vals)</span><br><span class="line">plt.plot(x_array, hinge_y_out, <span class="string">'b-'</span>, label=<span class="string">'Hinge Loss'</span>)</span><br><span class="line">plt.plot(x_array, xentropy_y_out, <span class="string">'r--'</span>, label=<span class="string">'Cross Entropy Loss'</span>)</span><br><span class="line">plt.plot(x_array, xentropy_sigmoid_y_out, <span class="string">'k-.'</span>, label=<span class="string">'Cross Entropy Sigmoid Loss'</span>)</span><br><span class="line">plt.plot(x_array, xentropy_weighted_y_out, <span class="string">'g:'</span>, label=<span class="string">'Weighted Cross Entropy Loss (x0.5)'</span>)</span><br><span class="line">plt.ylim(<span class="number">-1.5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#plt.xlim(-1, 3)</span></span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>, prop=&#123;<span class="string">'size'</span>: <span class="number">11</span>&#125;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li></ul><ol start="4"><li><p>总结<br>损失函数            使用类型         优点                       缺点<br>L2                 回归算法         更稳定                     缺少健壮<br>L1                 回归算法         更健壮                     缺少稳定<br>Psuedo-Huber       回归算法         更健壮、稳定                 参数多<br>Hinge              分类算法         常用于SVM的最大距离         异常值导致无边界损失<br>Cross-entropy      分类算法         更稳定                     缺少健壮，出现无边界损失</p></li><li><p>评价机器学习模型的其他指标<br>模型指标                     描述<br>R平方值<br>RMSE<br>混淆矩阵<br>召回率<br>精准度<br>F值</p></li></ol><h1 id="TensorFlow实现反向传播"><a href="#TensorFlow实现反向传播" class="headerlink" title="TensorFlow实现反向传播"></a>TensorFlow实现反向传播</h1><p>TensorFlow可以维护操作状态和基于反向传播自动地更新模型变量。通过计算图来更新变量，通过最小化损失函数来反向传播误差。实现方法是声明优化函数（optimization function）。一旦声明好优化函数，TensorFlow将通过优化函数在所有的计算图中解决反向传播的项。当传入数据和最小化损失函数，TensorFlow会在计算图中根据状态相应调节变量。</p><ol><li>回归算法举例<br>从均值为1，标准差为0.1的正态分布中抽样随机数，然后乘以变量A，损失函数为L2正则损失函数。理论上，A的最优值是10，因为生成的样例数据均值是1。</li></ol><ul><li>导入Python的数值计算模块，numpy和tensorflow：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure></li></ul><ul><li>创建计算图会话:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create graph</span></span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure></li></ul><ul><li>生成数据，创建占位符和变量A:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create data</span></span><br><span class="line">x_vals = np.random.normal(<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">100</span>)</span><br><span class="line">y_vals = np.repeat(<span class="number">10.</span>, <span class="number">100</span>)</span><br><span class="line">x_data = tf.placeholder(shape=[<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line">y_target = tf.placeholder(shape=[<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create variable (one model parameter = A)</span></span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></li></ul><ul><li>增加乘法操作：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add operation to graph</span></span><br><span class="line">my_output = tf.multiply(x_data, A)</span><br></pre></td></tr></table></figure></li></ul><ul><li>增加L2正则损失函数：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add L2 loss operation to graph</span></span><br><span class="line">loss = tf.square(my_output - y_target)</span><br></pre></td></tr></table></figure></li></ul><ul><li>在运行之前初始化变量：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize variables</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure></li></ul><ul><li>声明变量的优化器<br>大部分优化器算法需要知道每步迭代的步长，这距离是由学习率控制的。如果学习率太小，机器学习算法可能耗时很长才能收敛；如果学习率太大，机器学习算法可能会不收敛。相应地导致梯度消失和梯度爆炸等问题。学习率对算法的收敛影响较大。本次使用标准梯度下降法。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create Optimizer</span></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.02</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br></pre></td></tr></table></figure></li></ul><ul><li>训练算法<br>迭代101次，并且每25次迭代打印返回结果。选择一个随机的x和y，传入计算图中。TensorFlow将自动地计算损失，调整A偏差来最小化损失：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run Loop</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    rand_index = np.random.choice(<span class="number">100</span>)</span><br><span class="line">    rand_x = [x_vals[rand_index]]</span><br><span class="line">    rand_y = [y_vals[rand_index]]</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>)%<span class="number">25</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step #'</span> + str(i+<span class="number">1</span>) + <span class="string">' A = '</span> + str(sess.run(A)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))</span><br><span class="line"></span><br><span class="line">Here <span class="keyword">is</span> the output:</span><br><span class="line">Step <span class="comment">#25 A=[6.23402166]</span></span><br><span class="line">Loss=<span class="number">16.3173</span></span><br><span class="line">Step <span class="comment">#50 A=[8.50733757]</span></span><br><span class="line">Loss=<span class="number">3.56651</span></span><br><span class="line">Step <span class="comment">#75 A=[9.37753201]</span></span><br><span class="line">Loss=<span class="number">3.03149</span></span><br><span class="line">Step <span class="comment">#100 A=[9.80041122]</span></span><br><span class="line">Loss=<span class="number">0.0990248</span></span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li>分类算法举例<br>先重置一下之前的TensorFlow计算图，就可以使用相同的TensorFlow脚本继续分类算法的例子。我们试图找到一个优化的转换方法A，它可以把两个正态分布转换到原点，sigmoid函数将正态分布分割成不同的两类。</li></ol><ul><li>重置计算图，并且重新初始化变量:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line">ops.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create graph</span></span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure></li></ul><ul><li>从正态分布（N(-1,1)，N(3,1)）生成数据。同时也生成目标标签，占位符和偏差变量A：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create data</span></span><br><span class="line">x_vals = np.concatenate((np.random.normal(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">50</span>), np.random.normal(<span class="number">3</span>, <span class="number">1</span>, <span class="number">50</span>)))</span><br><span class="line">y_vals = np.concatenate((np.repeat(<span class="number">0.</span>, <span class="number">50</span>), np.repeat(<span class="number">1.</span>, <span class="number">50</span>)))</span><br><span class="line">x_data = tf.placeholder(shape=[<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line">y_target = tf.placeholder(shape=[<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create variable (one model parameter = A)</span></span><br><span class="line">A = tf.Variable(tf.random_normal(mean=<span class="number">10</span>, shape=[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></li></ul><ul><li>增加转换操作<br>这里不必封装sigmoid函数，因为损失函数中会实现此功能：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add operation to graph</span></span><br><span class="line"><span class="comment"># Want to create the operstion sigmoid(x + A)</span></span><br><span class="line"><span class="comment"># Note, the sigmoid() part is in the loss function</span></span><br><span class="line">my_output = tf.add(x_data, A)</span><br></pre></td></tr></table></figure></li></ul><ul><li>增加维度<br>由于指定的损失函数期望批量数据增加一个批量数的维度，使用expand_dims()函数增加维度：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Now we have to add another dimension to each (batch size of 1)</span></span><br><span class="line">my_output_expanded = tf.expand_dims(my_output, <span class="number">0</span>)</span><br><span class="line">y_target_expanded = tf.expand_dims(y_target, <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li>初始化变量A：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize variables</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure></li></ul><ul><li>声明损失函数<br>这里使用一个带非归一化logits的交叉熵的损失函数，同时会用sigmoid函数转换。TensorFlow的nn.sigmoid_cross_entropy_with_logits()函数实现所有这些功能，需要向它传入指定的维度：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add classification loss (cross entropy)</span></span><br><span class="line">xentropy = tf.nn.sigmoid_cross_entropy_with_logits(my_output_expanded, y_target_expanded)</span><br></pre></td></tr></table></figure></li></ul><ul><li>增加一个优化器函数让TensorFlow知道如何更新和偏差变量：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create Optimizer</span></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.05</span>)</span><br><span class="line">train_step = my_opt.minimize(xentropy)</span><br></pre></td></tr></table></figure></li></ul><ul><li>最后，通过随机选择的数据迭代几百次，相应地更新变量A。每迭代200次打印出损失和变量A的返回值：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run loop</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1400</span>):</span><br><span class="line">    rand_index = np.random.choice(<span class="number">100</span>)</span><br><span class="line">    rand_x = [x_vals[rand_index]]</span><br><span class="line">    rand_y = [y_vals[rand_index]]</span><br><span class="line">    </span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>)%<span class="number">200</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step #'</span> + str(i+<span class="number">1</span>) + <span class="string">' A = '</span> + str(sess.run(A)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(sess.run(xentropy, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))</span><br><span class="line"></span><br><span class="line">Step <span class="comment">#200 A=[3.59597969]</span></span><br><span class="line">Loss=[[<span class="number">0.00126199</span>]]</span><br><span class="line">Step <span class="comment">#400 A=[0.50947344]</span></span><br><span class="line">Loss=[[<span class="number">0.01149425</span>]]</span><br><span class="line">Step <span class="comment">#600 A=[-0.50994617]</span></span><br><span class="line">Loss=[[<span class="number">0.14271219</span>]]</span><br><span class="line">Step <span class="comment">#800 A=[-0.76606178]</span></span><br><span class="line">Loss=[[<span class="number">0.18807337</span>]]</span><br><span class="line">Step <span class="comment">#1000 A=[-0.90859312]</span></span><br><span class="line">Loss=[[<span class="number">0.02346182</span>]]</span><br><span class="line">Step <span class="comment">#1200 A=[-0.86169094]</span></span><br><span class="line">Loss=[[<span class="number">0.05427232</span>]]</span><br><span class="line">Step <span class="comment">#1400 A=[-1.08486211]</span></span><br><span class="line">Loss=[[<span class="number">0.04099189</span>]]</span><br></pre></td></tr></table></figure></li></ul><ol start="3"><li>总结</li></ol><ul><li>实现反向传播主要步骤：<ol><li>生成数据。</li><li>初始化占位符和变量。</li><li>创建损失函数。</li><li>定义一个优化器算法。</li><li>最后，通过随机数据样本进行迭代，更新变量。</li></ol></li></ul><ul><li>学习率选择方法：<br>学习率       优缺点                 使用场景<br>小学习率     收敛慢，但结果精确      若算法不稳定，先降低学习率<br>大学习率     结果不精确，但收敛快    若算法收敛太慢，可提高学习率<br>有时，标准梯度下降算法会明显卡顿或者收敛变慢，特别是在梯度为0附近的点。为此，TensorFlow的MomentumOptimizer()函数增加了一项势能，前一次迭代过程的梯度下降值的倒数。<br>另一个可以改变的是优化器的步长。理想情况下，对于变化小的变量使用大步长；而变化迅速的变量使用小步长。实现这种优点的常用算法是Adagrad算法。此算法考虑整个历史迭代的变量梯度，TensorFlow中相应功能的实现是AdagradOptimizer()函数。<br>有时，由于Adagrad算法计算整个历史迭代的梯度，导致梯度迅速变为0.解决这个局限性的是Adadelta算法，它限制使用的迭代次数。TensorFlow中相应功能的实现是AdadeltaOptimizer()函数。</li></ul><h1 id="TensorFlow实现随机训练和批量训练"><a href="#TensorFlow实现随机训练和批量训练" class="headerlink" title="TensorFlow实现随机训练和批量训练"></a>TensorFlow实现随机训练和批量训练</h1><p>扩展之前回归算法的例子——使用随机训练和批量训练</p><ol><li><p>导入numpy、matplotlib和tensorflow模块，开始一个计算图会话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure></li><li><p>声明批量大小<br>批量大小是指通过计算图一次传入多少训练数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_size=<span class="number">20</span></span><br></pre></td></tr></table></figure></li><li><p>声明模型的数据，占位符和变量<br>占位符有两个维度：第一个维度为None，第二个维度是批量训练中的数据量。我们能显示地设置维度为20，也能设为None。必须知道训练模型中的维度，这会阻止不合法的矩阵操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create data</span></span><br><span class="line">x_vals = np.random.normal(<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">100</span>)</span><br><span class="line">y_vals = np.repeat(<span class="number">10.</span>, <span class="number">100</span>)</span><br><span class="line">x_data = tf.placeholder(shape=[<span class="keyword">None</span>,<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line">y_target = tf.placeholder(shape=[<span class="keyword">None</span>,<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create variable (one model parameter = A)</span></span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></li></ol><h1 id="奥义：复制粘贴调试"><a href="#奥义：复制粘贴调试" class="headerlink" title="奥义：复制粘贴调试"></a>奥义：复制粘贴调试</h1><ol start="4"><li><p>在计算图中增加矩阵乘法操作<br>矩阵乘法不满足交换律，所以在matmul()函数中的矩阵参数顺序要正确：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add operation to graph</span></span><br><span class="line">my_output = tf.matmul(x_data, A)</span><br></pre></td></tr></table></figure></li><li><p>改变损失函数<br>批量训练时损失函数是每个数据点L2损失的平均值。在TensorFlow中通过reduce_mean()函数即可实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add L2 loss operation to graph</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(my_output - y_target))</span><br></pre></td></tr></table></figure></li><li><p>声明优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h1 id="TensorFlow实现创建分类器"><a href="#TensorFlow实现创建分类器" class="headerlink" title="TensorFlow实现创建分类器"></a>TensorFlow实现创建分类器</h1><h1 id="TensorFlow实现模型评估"><a href="#TensorFlow实现模型评估" class="headerlink" title="TensorFlow实现模型评估"></a>TensorFlow实现模型评估</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;计算图中的操作&quot;&gt;&lt;a href=&quot;#计算图中的操作&quot; class=&quot;headerlink&quot; title=&quot;计算图中的操作&quot;&gt;&lt;/a&gt;计算图中的操作&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>前端招聘题集（1）</title>
    <link href="http://yoursite.com/2018/04/21/front-end-interview-1/"/>
    <id>http://yoursite.com/2018/04/21/front-end-interview-1/</id>
    <published>2018-04-21T09:58:01.000Z</published>
    <updated>2018-04-21T13:37:51.100Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2018-3美团春招面试（答案待更新）"><a href="#2018-3美团春招面试（答案待更新）" class="headerlink" title="2018.3美团春招面试（答案待更新）"></a>2018.3美团春招面试（答案待更新）</h1><ol><li>cookie的使用</li><li>koa中间件实现原理</li><li>描述快速排序的实现</li><li>原型链相关问题</li><li>react生命周期</li><li>react性能优化</li><li>vue双面绑定原理</li><li>如何用js实现动画</li><li>css动画性能以及与js动画性能比较</li><li>二叉树</li><li>二叉树后序排序</li><li>模板引擎实现原理</li><li>怎么做同构以及同构的两份代码的差异性</li><li>koa中间件执行顺序以及如何实现</li><li>跨域问题</li><li>jsonp的原理以及优缺点</li><li>jquery和vue性能比较以及使用场景</li><li>什么是高阶组件</li><li>假设我维护一个服务端渲染框架，如何不侵入用户代码的情况下通知用户代码错误点（开放题）</li><li>js bridge原理</li><li>https和http的不同之处</li><li>http2.0的特性</li><li>如何实现一个promise</li><li>用node.js做过什么</li><li>graghQL和RESTful api</li></ol><h1 id="2018-4腾讯春招面试"><a href="#2018-4腾讯春招面试" class="headerlink" title="2018.4腾讯春招面试"></a>2018.4腾讯春招面试</h1><ol><li><p>node.js如何开启一个http服务</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//引入内置http模块</span></span><br><span class="line"><span class="keyword">var</span> http=<span class="built_in">require</span>(<span class="string">'http'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建一个简单的服务器，访问http://127.0.0.1:1337/，显示Hello World</span></span><br><span class="line">http.createServer(<span class="function"><span class="keyword">function</span>(<span class="params">req,res</span>)</span>&#123;</span><br><span class="line">    res.writeHead(<span class="number">200</span>,&#123;<span class="string">'Content-Type'</span>:<span class="string">'text/plain'</span>&#125;);</span><br><span class="line">    res.end(<span class="string">'Hello World\n'</span>);</span><br><span class="line">&#125;).listen(<span class="number">1337</span>,<span class="string">'127.0.0.1'</span>);</span><br><span class="line"><span class="built_in">console</span>.log(<span class="string">'Server running at http://127.0.0.1:137'</span>);</span><br></pre></td></tr></table></figure></li><li><p>CSS3动画的实现方式有哪些？动手写一下将一个div在1s内移动300px<br>transition方式：过渡动画，只定义初始和最终状态；<br>animation方式：可以逐帧设置</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"viewport"</span> <span class="attr">content</span>=<span class="string">"width=device-width,initial-scale=1.0"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"X-UA-Compatible"</span> <span class="attr">content</span>=<span class="string">"ie=edge"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Document<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">        /*transition属性动画结合transform变化属性，实现元素移动一段距离的动画*/</span></span><br><span class="line"><span class="undefined">        #transitionDiv:hover&#123;</span></span><br><span class="line"><span class="undefined">            transition:all 1s ease-in-out;</span></span><br><span class="line"><span class="undefined">            -webkit-transition:all 1s ease-in-out;</span></span><br><span class="line"><span class="undefined">            -moz-transition:all 1s ease-in-out;</span></span><br><span class="line"><span class="undefined">            -o-transition:all 1s ease-in-out;</span></span><br><span class="line"><span class="undefined"></span></span><br><span class="line"><span class="undefined">            transform:translateX(300px);</span></span><br><span class="line"><span class="undefined">            -ms-transform:translateX(300px);</span></span><br><span class="line"><span class="undefined">            -moz-transform:translateX(300px);</span></span><br><span class="line"><span class="undefined">            -webkit-transform:translateX(300px);</span></span><br><span class="line"><span class="undefined">            -o-transform:translateX(300px);</span></span><br><span class="line"><span class="undefined">        &#125;</span></span><br><span class="line"><span class="undefined"></span></span><br><span class="line"><span class="undefined">        /*通过animation属性，实现逐帧动画*/</span></span><br><span class="line"><span class="undefined">        #animationDiv:hover&#123;</span></span><br><span class="line"><span class="undefined">            animation:animName 1s ease-in-out;</span></span><br><span class="line"><span class="undefined">            -webkit-animation:animName 1s ease-in-out;</span></span><br><span class="line"><span class="undefined">            -moz-animation:animName 1s ease-in-out;</span></span><br><span class="line"><span class="undefined">            -o-animation:animName 1s ease-in-out;</span></span><br><span class="line"><span class="undefined">        &#125;</span></span><br><span class="line"><span class="undefined"></span></span><br><span class="line"><span class="undefined">        /*定义关键帧*/</span></span><br><span class="line"><span class="undefined">        @keyframes animName&#123;</span></span><br><span class="line"><span class="undefined">            0%&#123;</span></span><br><span class="line"><span class="undefined">                transform:translateX(0px);</span></span><br><span class="line"><span class="undefined">            &#125;</span></span><br><span class="line"><span class="undefined">            30%&#123;</span></span><br><span class="line"><span class="undefined">                transform:translateX(100px);</span></span><br><span class="line"><span class="undefined">            &#125;</span></span><br><span class="line"><span class="undefined">            60%&#123;</span></span><br><span class="line"><span class="undefined">                transform:translateX(200px);</span></span><br><span class="line"><span class="undefined">            &#125;</span></span><br><span class="line"><span class="undefined">            100%&#123;</span></span><br><span class="line"><span class="undefined">                transform:translateX(300px);</span></span><br><span class="line"><span class="undefined">            &#125;</span></span><br><span class="line"><span class="undefined">        &#125;</span></span><br><span class="line"><span class="undefined">    </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"transitionDiv"</span> <span class="attr">style</span>=<span class="string">"width:40px;height:40px;background-color:red;"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"animationDiv"</span> <span class="attr">style</span>=<span class="string">"width:40px;height:40px;background-color:green;"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>DNS解析过程？若是新申请的域名如何查找DNS？<br>DNS是应用层协议，事实上他是为其他应用层协议工作的，包括不限于HTTP和SMTP以及FTP，用于将用户提供的主机名解析为ip地址。<br>具体过程如下：</p></li></ol><ul><li>浏览器缓存: 当用户通过浏览器访问某域名时，浏览器首先会在自己的缓存中查找是否有该域名对应的IP地址（若曾经访问过该域名且没有清空缓存便存在)；</li></ul><ul><li>系统缓存： 当浏览器缓存中无域名对应IP则会自动检查用户计算机系统Hosts文件DNS缓存是否有该域名对应IP；</li></ul><ul><li>路由器缓存: 当浏览器及系统缓存中均无域名对应IP则进入路由器缓存中检查，以上三步均为客户端的DNS缓存；</li></ul><ul><li>ISP（互联网服务提供商）DNS缓存: 当在用户客服端查找不到域名对应IP地址，则将进入ISP DNS缓存中进行查询。比如你用的是电信的网络，则会进入电信的DNS缓存服务器中进行查找；(或者向网络设置中指定的local DNS进行查询，如果在PC指定了DNS的话，如果没有设置比如DNS动态获取，则向ISP DNS发起查询请求)</li></ul><ul><li>根域名服务器: 当以上均未完成，则进入根服务器进行查询。全球仅有13台根域名服务器，1个主根域名服务器，其余12为辅根域名服务器。根域名收到请求后会查看区域文件记录，若无则将其管辖范围内顶级域名（如.com）服务器IP告诉本地DNS服务器；</li></ul><ul><li>顶级域名服务器: 顶级域名服务器收到请求后查看区域文件记录，若无则将其管辖范围内主域名服务器的IP地址告诉本地DNS服务器；</li></ul><ul><li>主域名服务器: 主域名服务器接受到请求后查询自己的缓存，如果没有则进入下一级域名服务器进行查找，并重复该步骤直至找到正确记录；</li></ul><ul><li>保存结果至缓存: 本地域名服务器把返回的结果保存到缓存，以备下一次使用，同时将该结果反馈给客户端，客户端通过这个IP地址与web服务器建立链接。</li></ul><ol start="4"><li>Ajax请求状态以及意义<br>在javascript里面写AJax的时，最关键的一步是对XMLHttpRequest对象建立监听，即使用“onreadystatechange”方法。监听的时候，要对XMLHttpRequest对象的请求状态进行判断，通常是判断readyState的值为4且http返回状态status的值为200或者304时执行我们需要的操作。<br>readyState属性表示Ajax请求的当前状态。<br>0 代表未初始化。还没有调用open方法<br>1 代表正在加载。open 方法已被调用，但send方法还没有被调用<br>2 代表已加载完毕。send已被调用。请求已经开始<br>3 代表交互中。服务器正在发送响应<br>4 代表完成。响应发送完毕</li><li>cookie的操作，读写<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line"> <span class="keyword">var</span> cookieObj = &#123;</span><br><span class="line">    <span class="comment">//修改或是添加cookie</span></span><br><span class="line">   <span class="string">'add'</span>: <span class="function"><span class="keyword">function</span>(<span class="params">name, value, hours</span>) </span>&#123; </span><br><span class="line">        <span class="keyword">var</span> expire = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">if</span>(hours != <span class="literal">null</span>)&#123;</span><br><span class="line">            expire = <span class="keyword">new</span> <span class="built_in">Date</span>((<span class="keyword">new</span> <span class="built_in">Date</span>()).getTime() + hours * <span class="number">3600000</span>);</span><br><span class="line">            expire = <span class="string">"; expires="</span> + expire.toGMTString();</span><br><span class="line">        &#125;    </span><br><span class="line">    <span class="built_in">document</span>.cookie = name + <span class="string">"="</span> + <span class="built_in">escape</span>(value) + expire + <span class="string">";path=/"</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//如果指定域名可以使用如下</span></span><br><span class="line">    <span class="comment">//document.cookie = name + "=" + escape(value) + expire + ";path=/;domain=findme.wang";</span></span><br><span class="line">   &#125;,</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//读取cookie</span></span><br><span class="line">   <span class="string">'get'</span>: <span class="function"><span class="keyword">function</span>(<span class="params">c_name</span>)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">document</span>.cookie.length&gt;<span class="number">0</span>) &#123;</span><br><span class="line">            c_start = <span class="built_in">document</span>.cookie.indexOf(c_name + <span class="string">"="</span>);</span><br><span class="line">            <span class="keyword">if</span> (c_start != <span class="number">-1</span>) &#123; </span><br><span class="line">                c_start=c_start + c_name.length+<span class="number">1</span>;</span><br><span class="line">                c_end=<span class="built_in">document</span>.cookie.indexOf(<span class="string">";"</span>,c_start);</span><br><span class="line">                <span class="keyword">if</span> (c_end == <span class="number">-1</span>) &#123;</span><br><span class="line">                    c_end = <span class="built_in">document</span>.cookie.length;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">unescape</span>(<span class="built_in">document</span>.cookie.substring(c_start,c_end));</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;;</span><br><span class="line"> <span class="built_in">window</span>.cookieObj=cookieObj;</span><br><span class="line">&#125;());</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;2018-3美团春招面试（答案待更新）&quot;&gt;&lt;a href=&quot;#2018-3美团春招面试（答案待更新）&quot; class=&quot;headerlink&quot; title=&quot;2018.3美团春招面试（答案待更新）&quot;&gt;&lt;/a&gt;2018.3美团春招面试（答案待更新）&lt;/h1&gt;&lt;ol&gt;

      
    
    </summary>
    
      <category term="招聘试题" scheme="http://yoursite.com/categories/%E6%8B%9B%E8%81%98%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="JavaScript" scheme="http://yoursite.com/tags/JavaScript/"/>
    
      <category term="前端" scheme="http://yoursite.com/tags/%E5%89%8D%E7%AB%AF/"/>
    
      <category term="HTML" scheme="http://yoursite.com/tags/HTML/"/>
    
      <category term="CSS" scheme="http://yoursite.com/tags/CSS/"/>
    
      <category term="web" scheme="http://yoursite.com/tags/web/"/>
    
  </entry>
  
  <entry>
    <title>TFMLC学习笔记（1） TensorFlow基础</title>
    <link href="http://yoursite.com/2018/04/20/TFMLC-1/"/>
    <id>http://yoursite.com/2018/04/20/TFMLC-1/</id>
    <published>2018-04-19T18:41:11.000Z</published>
    <updated>2018-04-20T18:06:15.582Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TensorFlow算法的一般流程"><a href="#TensorFlow算法的一般流程" class="headerlink" title="TensorFlow算法的一般流程"></a>TensorFlow算法的一般流程</h1><ol><li>导入/生成样本数据集</li><li>转换和归一化数据</li></ol><ul><li>归一化函数如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=tf.nn.batch_norm_with_global_normalization(...)</span><br></pre></td></tr></table></figure></li></ul><ol start="3"><li>将样本数据集划为三块：训练样本集、测试样本集、验证样本集<br> 训练集和测试集要不同；用验证集决定最优的超参数</li><li>设置机器学习参数（超参数）</li></ol><ul><li>经常一次性初始化所有的机器学习参数，例如：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate=<span class="number">0.01</span></span><br><span class="line">batch_size=<span class="number">100</span></span><br><span class="line">iterations=<span class="number">1000</span></span><br></pre></td></tr></table></figure></li></ul><ol start="5"><li>初始化变量和占位符</li></ol><ul><li>在求解最优化工程中（最小化损失函数），TensorFlow使用占位符获取数据，并调整变量和权重/偏差；</li></ul><ul><li>TensorFlow指定数据大小和数据类型化初始化变量和占位符；</li></ul><ul><li>使用的数据类型字节数越多，结果越精确，运行速度也越慢；<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a_var=tf.constant(<span class="number">42</span>)</span><br><span class="line">x_input=tf.placeholder(tf.float32,[<span class="keyword">None</span>,input_size])</span><br><span class="line">y_input=tf.placeholder(tf.float32,[<span class="keyword">None</span>,num_classes])</span><br></pre></td></tr></table></figure></li></ul><ol start="6"><li>定义模型结构</li></ol><ul><li>一个简单的线性模型：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred=tf.add(tf.mul(x_input,weight_matrix),b_matrix)</span><br></pre></td></tr></table></figure></li></ul><ol start="7"><li>声明损失函数</li></ol><ul><li>损失函数说明预测值与实际值之间的差距:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss=tf.reduce_mean(tf.square(y_actual-y_pred))</span><br></pre></td></tr></table></figure></li></ul><ol start="8"><li>初始化模型和训练模型</li></ol><ul><li>TensorFlow创建计算图实例，通过占位符赋值，维护变量的状态信息</li><li><p>初始化计算图的一种方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">...</span><br><span class="line">session.run(...)</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><p>初始化计算图的另一种方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session=tf.Session(graph=graph)</span><br><span class="line">session.run(...)</span><br></pre></td></tr></table></figure></li></ul><ol start="9"><li>评估机器学习模型</li><li>调优超参数</li><li>发布/预测结果</li></ol><h1 id="声明张量"><a href="#声明张量" class="headerlink" title="声明张量"></a>声明张量</h1><p>TensorFlow主要的数据结构就是张量，它用张量来操作计算图。可以把变量或者占位符声明为张量。</p><ol><li>创建张量</li></ol><ul><li><p>固定张量</p><ul><li>创建指定维度的零张量：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zero_tsr=tf.zeros([row_dim,col_dim])</span><br></pre></td></tr></table></figure></li></ul><ul><li>创建指定维度的单位张量：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ones_tsr=tf.ones([row_dim,col_dim])</span><br></pre></td></tr></table></figure></li></ul><ul><li>创建指定维度的常数填充的张量：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filled_tsr=tf.fill([row_dim,col_dim],<span class="number">42</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li>用已知常数张量创建一个张量:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">constant_tsr=tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure></li></ul><ul><li>tf.constant()函数也可以广播一个值为数组，然后模拟tf.fill()函数的功能：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.constant(<span class="number">42</span>,[row_dim,col_dim])</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li>相似形状的张量<br>  新建一个与给定的tensor类型大小一致的tensor，其所有元素为0或1：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zeros_similar=tf.zeros_like(constant_tsr)</span><br><span class="line">ones_similar=tf.ones_like(contant_tsr)</span><br></pre></td></tr></table></figure></li></ul><ul><li>序列张量<br>  TensorFlow可以创建指定间隔的张量，以下函数的输出与range()函数和numpy中的linspace()函数的输出相似：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">linear_tsr=tf.linspace(start=<span class="number">0</span>,stop=<span class="number">1</span>,start=<span class="number">3</span>)          <span class="comment">#返回张量：[0.0,0.5,1.0]序列</span></span><br><span class="line">integer_seq_tsr=tf.range(start=<span class="number">6</span>,limit=<span class="number">15</span>,delta=<span class="number">3</span>)      <span class="comment">#返回张量：[6,9,12]序列</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>随机张量</p><ul><li><p>tf.randon_uniform()生成均匀分布的随机数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">randunif_tsr=tf.random_uniform([row_dim,col_dim],minval=<span class="number">0</span>,maxval=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>tf.randon_normal()生成正太分布的随机数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">random_tsr=tf.random_normal([row_dim,col_dim],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure></li><li><p>tf.truncated_normal()生成带有指定边界的正太分布的随机数，其正态分布的随机数位于指定均值（期望）到两个标准差之间的区间：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runcnorm_tsr=tf.truncated_normal([row_dim,col_dim],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure></li><li><p>tf.random_shuffle()和tf.random_crop()可以实现张量/数组的随机化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shuffled_output=tf.random_shuffle(input_tenspr)</span><br><span class="line">cropped_output=tf.random_crop(input_tensor,crop_size)</span><br></pre></td></tr></table></figure></li><li><p>tf.random_crop()可以实现对张量指定大小的随机剪裁，为了固定剪裁结果的一个维度，需要在相应的维度上赋其最大值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cropped_image=tf.random_crop(my_image,[height/<span class="number">2</span>,width/<span class="number">2</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure></li></ul></li></ul><ol start="2"><li>把张量封装为变量:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mt_var=tf.Variable(tf.zeros([row_dim,col_dim]))</span><br></pre></td></tr></table></figure></li></ol><h1 id="使用占位符和变量"><a href="#使用占位符和变量" class="headerlink" title="使用占位符和变量"></a>使用占位符和变量</h1><p>变量是TensorFlow机器学习算法的参数，TensorFlow维护（调整）这些变量的状态来优化机器学习算法。占位符是TensorFlow对象，用于表示输入输出数据的格式，允许传入指定类型和形状的数据，并依赖计算图的计算结果。</p><ol><li><p>使用tf.Variable()函数创建变量，输入一个张量，返回一个变量。声明变量之后需要初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">my_var=tf.Variable(tf.zeros([<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line">sess=tf.Session()</span><br><span class="line">initialize_op=tf.global_variable_initializer()</span><br><span class="line">sess.run(initialize_op)</span><br></pre></td></tr></table></figure></li><li><p>占位符仅仅声明数据位置，用于传入数据到计算图。占位符通过会话的feed_dict参数获取数据。在计算图中使用占位符时，必须在其上执行至少一个操作。在TensorFlow中，初始化计算图，声明一个占位符x，定义y为x的identity操作。identity操作返回占位符传入的数据本身。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sess=tf.Session()</span><br><span class="line">x=tf.placeholder(tf.float32,shape=[<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">y=tf.identity(x)</span><br><span class="line">x_vals=np.random.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">sess.run(y,feed_dict=&#123;x:x_vals&#125;)</span><br><span class="line"><span class="comment"># Note that sess.run(x,feed_dict=&#123;x:x_vals&#125;) will result in a self-referencing error.</span></span><br></pre></td></tr></table></figure></li></ol><h1 id="操作矩阵"><a href="#操作矩阵" class="headerlink" title="操作矩阵"></a>操作矩阵</h1><ul><li>创建一个图会话：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess=tf.Session()</span><br></pre></td></tr></table></figure></li></ul><ul><li>创建矩阵：<ul><li>使用numpy创建二维矩阵</li><li>使用创建张量的函数（zeros(),ones(),truncated_normal()等），并为其指定一个二维形状 </li><li>使用diag()函数从一个一维矩阵来创建对角矩阵<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">identity_matrix=tf.diag([<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line">A=tf.truncated_normal([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">B=tf.fill([<span class="number">2</span>,<span class="number">3</span>],<span class="number">5.0</span>)</span><br><span class="line">C=tf.random_uniform([<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line">D=tf.convert_to_tensor(np.array([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">-3.</span>,<span class="number">-7.</span>,<span class="number">-1.</span>],[<span class="number">0.</span>,<span class="number">5.</span>,<span class="number">-2.</span>,]]))</span><br><span class="line">print(sess.run(identity_matrix))</span><br><span class="line">[[<span class="number">1.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">0.</span>  <span class="number">1.</span>]]</span><br><span class="line">print(sess.run(A))</span><br><span class="line">[[ <span class="number">0.96751703</span>   <span class="number">0.11397751</span>  <span class="number">-0.3438891</span>]</span><br><span class="line"> [<span class="number">-0.10132604</span>  <span class="number">-0.8432678</span>   <span class="number">0.29810596</span>]]</span><br><span class="line">print(sess.run(B))</span><br><span class="line">[[<span class="number">5.</span>  <span class="number">5.</span>  <span class="number">5.</span> ]</span><br><span class="line"> [<span class="number">5.</span>  <span class="number">5.</span>  <span class="number">5.</span>]]</span><br><span class="line">print(sess.run(C))</span><br><span class="line">[[<span class="number">0.33184157</span>  <span class="number">0.08907614</span>]</span><br><span class="line"> [<span class="number">0.53189191</span>  <span class="number">0.67605299</span>]</span><br><span class="line"> [<span class="number">0.95889051</span>  <span class="number">0.67061249</span>]]</span><br><span class="line">print(sess.run(D))</span><br><span class="line">[[ <span class="number">1.</span>   <span class="number">2.</span>  <span class="number">3.</span>]</span><br><span class="line"> [<span class="number">-3.</span>  <span class="number">-7.</span> <span class="number">-1.</span>]</span><br><span class="line"> [ <span class="number">0.</span>   <span class="number">5.</span> <span class="number">-2.</span>]]</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li>矩阵的加减法：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(A+B))</span><br><span class="line">[[<span class="number">4.61596632</span>  <span class="number">5.39771316</span>   <span class="number">4.4325695</span>]</span><br><span class="line"> [<span class="number">3.26702736</span>  <span class="number">5.14477345</span>  <span class="number">4.98265553</span>]]</span><br><span class="line">print(sess.run(B-B))</span><br><span class="line">[[<span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure></li></ul><ul><li>矩阵的乘法：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.matmul(B,identity_matrix)))</span><br><span class="line">[[<span class="number">5.</span>  <span class="number">5.</span>  <span class="number">5.</span>]</span><br><span class="line"> [<span class="number">5.</span>  <span class="number">5.</span>  <span class="number">5.</span>]]</span><br></pre></td></tr></table></figure></li></ul><ul><li>矩阵转置：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.transpose(C)))</span><br><span class="line">[[<span class="number">0.67124544</span>  <span class="number">0.26766731</span>  <span class="number">0.99068872</span>]</span><br><span class="line"> [<span class="number">0.25006068</span>  <span class="number">0.86560275</span>  <span class="number">0.58411312</span>]]</span><br></pre></td></tr></table></figure></li></ul><ul><li>矩阵行列式的使用：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.matrix_determinant(D)))</span><br><span class="line"><span class="number">-38.0</span></span><br></pre></td></tr></table></figure></li></ul><ul><li>逆矩阵：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.matrix_inverse(D)))</span><br><span class="line">[[<span class="number">-0.5</span>        <span class="number">-0.5</span>        <span class="number">-0.5</span>      ]</span><br><span class="line"> [<span class="number">0.15789474</span>  <span class="number">0.05263158</span>  <span class="number">0.21052632</span>]</span><br><span class="line"> [<span class="number">0.39473684</span>  <span class="number">0.13157895</span>  <span class="number">0.02631579</span>]]</span><br></pre></td></tr></table></figure></li></ul><ul><li>矩阵的分解：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.cholesky(identity_matrix)))</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure></li></ul><ul><li>矩阵的特征值和特征向量：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.self_adjoint_eig(D)))     <span class="comment">#第一行为特征值，剩余的向量是对应的特征向量</span></span><br><span class="line">[[<span class="number">-10.65907521</span> <span class="number">-0.22750691</span>  <span class="number">2.88658212</span>]</span><br><span class="line"> [  <span class="number">0.21749542</span>  <span class="number">0.63250104</span> <span class="number">-0.74339638</span>]</span><br><span class="line"> [  <span class="number">0.84526515</span>  <span class="number">0.2587998</span>   <span class="number">0.46749277</span>]</span><br><span class="line"> [  <span class="number">-0.4880805</span>  <span class="number">0.73004459</span>  <span class="number">0.47834331</span>]]</span><br></pre></td></tr></table></figure></li></ul><h1 id="张量的基本操作"><a href="#张量的基本操作" class="headerlink" title="张量的基本操作"></a>张量的基本操作</h1><p>TensorFlow张量的基本操作有：add()、sub()、mul()、div()、mod()</p><ul><li>值得注意的是，div()函数返回值的数据类型与输入类型一致。所以，在Python2中，整数除法的实际返回是商的向下取整；而在Python3中，TensorFlow提供truediv()函数，其会在除法操作前强制转换整数为浮点数，所以最终的除法结果是浮点数。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.div(<span class="number">3</span>,<span class="number">4</span>)))</span><br><span class="line"><span class="number">0</span></span><br><span class="line">print(sess.run(tf.truediv(<span class="number">3</span>,<span class="number">4</span>)))</span><br><span class="line"><span class="number">0.75</span></span><br></pre></td></tr></table></figure></li></ul><ul><li>如果要对浮点数进行整数除法，可以使用floordiv()函数。此函数也返回浮点数结果，但是其会向下舍去小数位到最近的整数。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.floordiv(<span class="number">3.0</span>,<span class="number">4.0</span>)))</span><br><span class="line"><span class="number">0.0</span></span><br></pre></td></tr></table></figure></li></ul><ul><li>cross()函数用来计算两个张量间的点积。点积只为三维向量而定义，所以cross()函数的输入是两个三维向量：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.cross([<span class="number">1.</span>,<span class="number">0.</span>,<span class="number">0.</span>],[<span class="number">0.</span>,<span class="number">1.</span>,<span class="number">0.</span>])))</span><br><span class="line">[<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.0</span>]</span><br></pre></td></tr></table></figure></li></ul><ul><li>其他数学函数：<br>abs()<br>ceil()<br>cos()<br>exp()<br>floor()<br>inv()：返回输入参数张量的倒数<br>log()<br>maximum()<br>minimum()<br>neg()：返回输入参数张量的负值<br>pow()：返回输入参数第一个张量的第二个张量的次幂<br>round()：返回输入参数张量的四舍五入结果<br>rsqrt()：返回输入参数张量的平方根的倒数<br>sign()：根据输入参数张量的符号，返回-1、0或1<br>sin()<br>sqrt()：返回输入参数张量的平方根<br>square()<br>digamma()：普西（Psi）函数，lgamma()函数的导数<br>erf()：返回张量的高斯误差函数<br>erfc()：返回张量的互补误差函数<br>igamma()：返回下不完全伽马函数<br>igammac()：返回上不完全伽马函数<br>lbeta()：返回贝塔函数绝对值的自然对数<br>lgamma()：返回伽马函数绝对值的自然对数<br>squared_difference()：返回两个张量间差值的平方</li></ul><h1 id="实现激励函数"><a href="#实现激励函数" class="headerlink" title="实现激励函数"></a>实现激励函数</h1><p>TensorFlow的激励函数位于神经网络（neural network，nn）库。</p><ol><li><p>整流线性单元（Rectifier Linear Unit，ReLU）<br>神经网络最常用的非线性函数。其函数为max(0,x)，连续但不平滑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.nn.relu([<span class="number">-3.</span>,<span class="number">3.</span>,<span class="number">10.</span>])))</span><br><span class="line">[<span class="number">0.</span>  <span class="number">3.</span>  <span class="number">10.</span>]</span><br></pre></td></tr></table></figure></li><li><p>ReLU6<br>有时为了抵消ReLU激励函数的线性增长部分，会在min()函数中嵌入max(0,x)，其在TensorFlow中的实现称为ReLU6，表示为min(max(0,x),6)。这是hard-sigmoid函数的变种，计算运行速度快，解决了梯度消失（无限趋近于0）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.nn.relu6([<span class="number">-3.</span>,<span class="number">3.</span>,<span class="number">10.</span>])))</span><br><span class="line">[<span class="number">0.</span>  <span class="number">3.</span>  <span class="number">6.</span>]</span><br></pre></td></tr></table></figure></li><li><p>sigmoid函数<br>最常用的连续，平滑的激励函数。它也被称作逻辑函数（Logistic函数），表示为1/(1+exp(-x))。sigmoid函数由于在机器学习训练过程中反向传播项趋近于0.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.nn.sigmoid([<span class="number">-1.</span>,<span class="number">0.</span>,<span class="number">1.</span>])))</span><br><span class="line">[<span class="number">0.26894143</span>  <span class="number">0.5</span>  <span class="number">0.7310586</span>]</span><br></pre></td></tr></table></figure></li><li><p>双曲正切函数（hyper tangent,tanh）<br>双曲正弦与双曲余弦的比值，另一种写法是(exp(x)-exp(-x))/(exp(x)+exp(-x))</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.nn.tanh([<span class="number">-1.</span>,<span class="number">0.</span>,<span class="number">1.</span>])))</span><br><span class="line">[<span class="number">-0.76159418</span>  <span class="number">0.</span>  <span class="number">0.76159418</span>]</span><br></pre></td></tr></table></figure></li><li><p>softsign函数<br>符号函数的连续估计，表达式：x/(abs(x)+1)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.nn.softsign([<span class="number">-1.</span>,<span class="number">0.</span>,<span class="number">-1.</span>])))</span><br><span class="line">[<span class="number">-0.5</span> <span class="number">0.</span> <span class="number">0.5</span>]</span><br></pre></td></tr></table></figure></li><li><p>softplus激励函数<br>ReLU激励函数的平滑版，表达式为log(exp(x)+1)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.nn.softplus([<span class="number">-1.</span>,<span class="number">0.</span>,<span class="number">-1.</span>])))</span><br><span class="line">[<span class="number">0.31326166</span> <span class="number">0.69314718</span> <span class="number">1.31326163</span>]</span><br></pre></td></tr></table></figure></li><li><p>ELU激励函数（Exponential Linear Unit,ELU）<br>与softplus激励函数相似，区别在于：当输入无限小时，ELU激励函数趋近于-1，而softplus激励函数趋近于0。表达式为(exp(x)+1) if x&lt;0 else x</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.nn.elu([<span class="number">-1.</span>,<span class="number">0.</span>,<span class="number">-1.</span>])))</span><br><span class="line">[<span class="number">-0.63212055</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br></pre></td></tr></table></figure></li></ol><h1 id="读取数据源"><a href="#读取数据源" class="headerlink" title="读取数据源"></a>读取数据源</h1><ol><li><p>鸢尾花卉数据集（Iris data）<br>此样本数据是机器学习和统计分析最经典的数据集，包含鸢尾花、变色鸢尾和维吉尼亚鸢尾各自的花萼和花瓣的长度和宽度。总共有150个数据集，每类有50个样本。用Python加载样本数据集时，可以使用Scikit Learn的数据集函数，使用方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">print(len(iris.data))</span><br><span class="line"><span class="number">150</span></span><br><span class="line">print(len(iris.target))</span><br><span class="line"><span class="number">150</span></span><br><span class="line">print(iris.data[<span class="number">0</span>])         <span class="comment">#Sepal length, Sepal width, Petal length, Petal width</span></span><br><span class="line">[<span class="number">5.1</span> <span class="number">3.5</span> <span class="number">1.4</span> <span class="number">0.2</span>]</span><br><span class="line">print(set(iris.target))     <span class="comment"># Ⅰ. setosa, Ⅱ. virginica, Ⅲ. versicolor</span></span><br><span class="line">&#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure></li><li><p>出生体重数据（Birth weight data）<br>此样本数据集是婴儿出生体重以及母亲和家庭历史人口统计学、医学指标，有189个样本集，包含11个特征变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">birthdata_url = <span class="string">'https://www.umass.edu/statdata/statdata/data/lowbwt.dat'</span></span><br><span class="line">birth_file = requests.get(birthdata_url)</span><br><span class="line">birth_data = birth_file.text.split(<span class="string">'\r\n'</span>)[<span class="number">5</span>:]</span><br><span class="line">birth_header = [x <span class="keyword">for</span> x <span class="keyword">in</span> birth_data[<span class="number">0</span>].split(<span class="string">' '</span>) <span class="keyword">if</span> len(x)&gt;=<span class="number">1</span>]</span><br><span class="line">birth_data = [[float(x) <span class="keyword">for</span> x <span class="keyword">in</span> y.split(<span class="string">' '</span>) <span class="keyword">if</span> len(x)&gt;=<span class="number">1</span>] <span class="keyword">for</span> y <span class="keyword">in</span> birth_data[<span class="number">1</span>:] <span class="keyword">if</span> len(y)&gt;=<span class="number">1</span>]</span><br><span class="line">print(len(birth_data))            <span class="comment">#189</span></span><br><span class="line">print(len(birth_data[<span class="number">0</span>]))         <span class="comment">#11</span></span><br></pre></td></tr></table></figure></li><li><p>波士顿房价数据（Boston Housing data）<br>此样本数据集保存在卡内基梅隆大学机器学习仓库，总共有506个房价样本，包含14个特征变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">housing_url = <span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'</span></span><br><span class="line">housing_header = [<span class="string">'CRIM'</span>, <span class="string">'ZN'</span>, <span class="string">'INDUS'</span>, <span class="string">'CHAS'</span>, <span class="string">'NOX'</span>, <span class="string">'RM'</span>, <span class="string">'AGE'</span>, <span class="string">'DIS'</span>, <span class="string">'RAD'</span>, <span class="string">'TAX'</span>, <span class="string">'PTRATIO'</span>, <span class="string">'B'</span>, <span class="string">'LSTAT'</span>, <span class="string">'MEDV'</span>]</span><br><span class="line">housing_file = requests.get(housing_url)</span><br><span class="line">housing_data = [[float(x) <span class="keyword">for</span> x <span class="keyword">in</span> y.split(<span class="string">' '</span>) <span class="keyword">if</span> len(x)&gt;=<span class="number">1</span>] <span class="keyword">for</span> y <span class="keyword">in</span> housing_file.text.split(<span class="string">'\n'</span>) <span class="keyword">if</span> len(y)&gt;=<span class="number">1</span>]</span><br><span class="line">print(len(housing_data))         <span class="comment">#506</span></span><br><span class="line">print(len(housing_data[<span class="number">0</span>]))      <span class="comment">#14</span></span><br></pre></td></tr></table></figure></li><li><p>MNIST手写体字库<br>MNIST手写体字库是NIST手写体字库的子样本数据集，网址：<a href="https://yann.lecun.com/exdb/mnist" target="_blank" rel="noopener">https://yann.lecun.com/exdb/mnist</a> 包含70000张0到9的图像，其中60000张标注为训练样本数据集，10000张为测试样本数据集。TensorFlow提供内建函数访问。MNIST常用来进行图像识别训练。为了预防过拟合，需要提供验证数据集。TensorFlow从训练样本数据集中留出5000张图片作为验证样本数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">print(len(mnist.train.images))          <span class="comment">#55000</span></span><br><span class="line">print(len(mnist.test.images))           <span class="comment">#10000</span></span><br><span class="line">print(len(mnist.validation.images))     <span class="comment">#5000</span></span><br><span class="line">print(mnist.train.labels[<span class="number">1</span>,:])          <span class="comment">#[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]</span></span><br></pre></td></tr></table></figure></li><li><p>垃圾短信文本数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get/read zip file</span></span><br><span class="line">zip_url = <span class="string">'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'</span></span><br><span class="line">r = requests.get(zip_url)</span><br><span class="line">z = ZipFile(io.BytesIO(r.content))</span><br><span class="line">file = z.read(<span class="string">'SMSSpamCollection'</span>)</span><br><span class="line"><span class="comment"># Format Data</span></span><br><span class="line">text_data = file.decode()</span><br><span class="line">text_data = text_data.encode(<span class="string">'ascii'</span>,errors=<span class="string">'ignore'</span>)</span><br><span class="line">text_data = text_data.decode().split(<span class="string">'\n'</span>)</span><br><span class="line">text_data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> text_data <span class="keyword">if</span> len(x)&gt;=<span class="number">1</span>]</span><br><span class="line">[text_data_target, text_data_train] = [list(x) <span class="keyword">for</span> x <span class="keyword">in</span> zip(*text_data)]</span><br><span class="line">print(len(text_data_train))           <span class="comment">#5574</span></span><br><span class="line">print(set(text_data_target))          <span class="comment">#&#123;'ham','spam'&#125;</span></span><br><span class="line">print(text_data_train[<span class="number">1</span>])             <span class="comment">#OK lar...Joking wif u oni</span></span><br></pre></td></tr></table></figure></li><li><p>影评样本数据集<br>此样本数据集是观影者的影评，分为好评和差评。位于康奈尔大学的仓库：<a href="http://www.cs.cornell.edu/people/pabo/movie-review-data" target="_blank" rel="noopener">http://www.cs.cornell.edu/people/pabo/movie-review-data</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"></span><br><span class="line">movie_data_url = <span class="string">'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'</span></span><br><span class="line">r = requests.get(movie_data_url)</span><br><span class="line"><span class="comment"># Stream data into temp object</span></span><br><span class="line">stream_data = io.BytesIO(r.content)</span><br><span class="line">tmp = io.BytesIO()</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    s = stream_data.read(<span class="number">16384</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> s:  </span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    tmp.write(s)</span><br><span class="line">stream_data.close()</span><br><span class="line">tmp.seek(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Extract tar file</span></span><br><span class="line">tar_file = tarfile.open(fileobj=tmp, mode=<span class="string">"r:gz"</span>)</span><br><span class="line">pos = tar_file.extractfile(<span class="string">'rt-polaritydata/rt-polarity.pos'</span>)</span><br><span class="line">neg = tar_file.extractfile(<span class="string">'rt-polaritydata/rt-polarity.neg'</span>)</span><br><span class="line"><span class="comment"># Save pos/neg reviews</span></span><br><span class="line">pos_data = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> pos:</span><br><span class="line">    pos_data.append(line.decode(<span class="string">'ISO-8859-1'</span>).encode(<span class="string">'ascii'</span>,errors=<span class="string">'ignore'</span>).decode())</span><br><span class="line">neg_data = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> neg:</span><br><span class="line">    neg_data.append(line.decode(<span class="string">'ISO-8859-1'</span>).encode(<span class="string">'ascii'</span>,errors=<span class="string">'ignore'</span>).decode())</span><br><span class="line">tar_file.close()</span><br><span class="line"></span><br><span class="line">print(len(pos_data))              <span class="comment">#5531</span></span><br><span class="line">print(len(neg_data))              <span class="comment">#5331</span></span><br><span class="line">print(neg_data[<span class="number">0</span>])                <span class="comment">#simplistic,silly and tedious</span></span><br></pre></td></tr></table></figure></li><li><p>莎士比亚著作文本数据集（Shakespeare text data）<br>此样本数据集是古登堡数字电子书计划提供的免费电子书籍，他们编译了莎士比亚所有著作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">shakespeare_url = <span class="string">'http://www.gutenberg.org/cache/epub/100/pg100.txt'</span></span><br><span class="line"><span class="comment"># Get Shakespeare text</span></span><br><span class="line">response = requests.get(shakespeare_url)</span><br><span class="line">shakespeare_file = response.content</span><br><span class="line"><span class="comment"># Decode binary into string</span></span><br><span class="line">shakespeare_text = shakespeare_file.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="comment"># Drop first few descriptive paragraphs.</span></span><br><span class="line">shakespeare_text = shakespeare_text[<span class="number">7675</span>:]</span><br><span class="line">print(len(shakespeare_text))            <span class="comment">#Number of characters：5582212</span></span><br></pre></td></tr></table></figure></li><li><p>英德句子翻译样本集<br>此数据集由在线翻译数据库Tatoeba发布，ManyThings.org整理并提供下载。这里提供英德语句互译的文本文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line">sentence_url = <span class="string">'http://www.manythings.org/anki/deu-eng.zip'</span></span><br><span class="line">r = requests.get(sentence_url)</span><br><span class="line">z = ZipFile(io.BytesIO(r.content))</span><br><span class="line">file = z.read(<span class="string">'deu.txt'</span>)</span><br><span class="line"><span class="comment"># Format Data</span></span><br><span class="line">eng_ger_data = file.decode()</span><br><span class="line">eng_ger_data = eng_ger_data.encode(<span class="string">'ascii'</span>,errors=<span class="string">'ignore'</span>)</span><br><span class="line">eng_ger_data = eng_ger_data.decode().split(<span class="string">'\n'</span>)</span><br><span class="line">eng_ger_data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> eng_ger_data <span class="keyword">if</span> len(x)&gt;=<span class="number">1</span>]</span><br><span class="line">[english_sentence, german_sentence] = [list(x) <span class="keyword">for</span> x <span class="keyword">in</span> zip(*eng_ger_data)]</span><br><span class="line">print(len(english_sentence))         <span class="comment">#137673</span></span><br><span class="line">print(len(german_sentence))          <span class="comment">#137673</span></span><br><span class="line">print(eng_ger_data[<span class="number">10</span>])              <span class="comment">#['I won!,' 'Ich habe gewonnen！']</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;TensorFlow算法的一般流程&quot;&gt;&lt;a href=&quot;#TensorFlow算法的一般流程&quot; class=&quot;headerlink&quot; title=&quot;TensorFlow算法的一般流程&quot;&gt;&lt;/a&gt;TensorFlow算法的一般流程&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;导入/生
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>TFMLC学习笔记（0）序言</title>
    <link href="http://yoursite.com/2018/04/20/TFMLC-0/"/>
    <id>http://yoursite.com/2018/04/20/TFMLC-0/</id>
    <published>2018-04-19T17:56:41.000Z</published>
    <updated>2018-04-19T18:39:56.538Z</updated>
    
    <content type="html"><![CDATA[<h1 id="书籍信息"><a href="#书籍信息" class="headerlink" title="书籍信息"></a>书籍信息</h1><ul><li>书名：TensorFlow机器学习实战指南</li></ul><ul><li>作者：[美]尼克·麦克卢尔</li></ul><ul><li>译者：曾益强</li></ul><ul><li>出版社：机械工业出版社</li></ul><h1 id="Book-Information"><a href="#Book-Information" class="headerlink" title="Book Information"></a>Book Information</h1><ul><li>Name: TensorFlow Machine Learning Cookbook</li></ul><ul><li>Author: Nike McClure</li></ul><ul><li>Publisher: Packt Publishing</li></ul><h1 id="环境搭建-软件清单"><a href="#环境搭建-软件清单" class="headerlink" title="环境搭建+软件清单"></a>环境搭建+软件清单</h1><ul><li>Python3</li></ul><ul><li>TensorFlow</li></ul><ul><li>numpy</li></ul><ul><li>scipy</li></ul><ul><li>sklearn</li></ul><ul><li>jupyter</li></ul><ul><li>matplotlib</li></ul><ul><li>requests</li></ul><ul><li>Pillow</li></ul><h1 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a>源码下载</h1><ul><li><a href="https://github.com/nfmcclure/tensorflow_cookbook" target="_blank" rel="noopener">GitHub</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;书籍信息&quot;&gt;&lt;a href=&quot;#书籍信息&quot; class=&quot;headerlink&quot; title=&quot;书籍信息&quot;&gt;&lt;/a&gt;书籍信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;书名：TensorFlow机器学习实战指南&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;作者：[美]尼克·麦克卢尔&lt;
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/04/19/hello-world/"/>
    <id>http://yoursite.com/2018/04/19/hello-world/</id>
    <published>2018-04-18T17:38:21.578Z</published>
    <updated>2018-04-18T17:38:21.579Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
