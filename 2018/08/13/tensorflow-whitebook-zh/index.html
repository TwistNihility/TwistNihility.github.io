<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-corner-indicator.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="TensorFlow,机器学习," />





  <link rel="alternate" href="/atom.xml" title="Sisyphus's Utopia" type="application/atom+xml" />






<meta name="description" content="TensorFlow：在异构分布式系统上的大规模机器学习（初版白皮书，2015年11月9日）向Google Rearch的大佬们低头：Mart`ın Abadi,Ashish Agarwal,Paul Barham,Eugene Brevdo,Zhifeng Chen,Craig Citro,Greg S.Corrado,Andy Davis,Jeffrey Dean,Matthieu Devin">
<meta name="keywords" content="TensorFlow,机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow白皮书（中文版）">
<meta property="og:url" content="http://yoursite.com/2018/08/13/tensorflow-whitebook-zh/index.html">
<meta property="og:site_name" content="Sisyphus&#39;s Utopia">
<meta property="og:description" content="TensorFlow：在异构分布式系统上的大规模机器学习（初版白皮书，2015年11月9日）向Google Rearch的大佬们低头：Mart`ın Abadi,Ashish Agarwal,Paul Barham,Eugene Brevdo,Zhifeng Chen,Craig Citro,Greg S.Corrado,Andy Davis,Jeffrey Dean,Matthieu Devin">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-08-29T14:24:19.360Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow白皮书（中文版）">
<meta name="twitter:description" content="TensorFlow：在异构分布式系统上的大规模机器学习（初版白皮书，2015年11月9日）向Google Rearch的大佬们低头：Mart`ın Abadi,Ashish Agarwal,Paul Barham,Eugene Brevdo,Zhifeng Chen,Craig Citro,Greg S.Corrado,Andy Davis,Jeffrey Dean,Matthieu Devin">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/08/13/tensorflow-whitebook-zh/"/>





  <title>TensorFlow白皮书（中文版） | Sisyphus's Utopia</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sisyphus's Utopia</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/13/tensorflow-whitebook-zh/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Twist Nihility">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/saber.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sisyphus's Utopia">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorFlow白皮书（中文版）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-13T16:16:54+08:00">
                2018-08-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/08/13/tensorflow-whitebook-zh/" class="leancloud_visitors" data-flag-title="TensorFlow白皮书（中文版）">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  14,643
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  57
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>TensorFlow：在异构分布式系统上的大规模机器学习<br>（初版白皮书，2015年11月9日）<br>向Google Rearch的大佬们低头：<br>Mart`ın Abadi,Ashish Agarwal,Paul Barham,Eugene Brevdo,Zhifeng Chen,Craig Citro,Greg S.Corrado,Andy Davis,Jeffrey Dean,Matthieu Devin,Sanjay Ghemawat,Ian Goodfellow,Andrew Harp,Geoffrey Irving,Michael Isard,Yangqing Jia,Rafal Jozefowicz,Lukasz Kaiser,Manjunath Kudlur,Josh Levenberg,Dan Mané,Rajat Monga,Sherry Moore,Derek Murray,Chris Olah,Mike Schuster,Jonathon Shlens,Benoit Steiner,Ilya Sutskever,Kunal Talwar,Paul Tucker,Vincent Vanhoucke,Vijay Vasudevan,Fernanda Viégas,Oriol Vinyals,Pete Warden,Martin Wattenberg,Martin Wicke,Yuan Yu,and Xiaoqiang Zheng</p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>&emsp;&emsp;TensorFlow是一个表现机器学习算法的接口和执行算法的实现框架。不需改动，或者只需改动少许，使用TensorFlow表示的计算就可以在众多异构的系统上方便地移植，从手机这样的移动设备或者平板电脑到成千的GPU计算集群。该系统十分灵活，可以表示很多的算法，包括深度神经网络的训练和推断算法，也已经被用在科学研究，以及将机器学习系统应用在计算机科学领域或者其他领域中，例如语言识别、计算机视觉、机器人、信息检索、自然语言处理、地理信息抽取和计算药物发现。此论文描述了TensorFlow的接口和我们在Google构建的接口的一种实现。TensorFlow API和一种参考实现都已经作为开源项目按照Apache 2.0协议于2015年11月在 <a href="http://www.tensorflow.org" target="_blank" rel="noopener">www.tensorflow.org</a> 上发布。</p>
<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><p>&emsp;&emsp;Google Brain计划开始于2011年，旨在探索超大规模深度神经网络的用处，既为了科研，也为了在Google产品中使用。作为这项计划的早期工作之一，我们构建了DistBelief——第一代可扩展分布式训练和推测系统，此系统运行良好。我们和其他Google的同事使用DistBelief做了大量的研究，包括无监督学习，语言表示，用于图像分类和目标检测的模型，视频分类，语音识别，序列预测，Go的移动选择，行人检测，强化学习等等。除此之外，在和Google Brain团队密切的合作中，超过50家Google内部和其他Alphabet公司已经在大量的产品上应用了DistBelief构建的深度神经网络，包括Google搜索，我们的广告产品，我们的语音识别系统，Google Photos，Google Maps和StreetView，Google翻译，YouTube等等。<br>&emsp;&emsp;基于我们对于DistBelief的经验，和某种对于训练和使用神经网络所需要的理想系统性能和要求更加完备的理解，我们构建了TensorFlow——第二代用于实现和部署大规模机器学习模型的系统。TensorFlow需要使用类似数据流模型来描绘计算，再把这些计算映射到大量不同的硬件平台上，从在诸如Android和iOS等移动设备平台上运行推断，到使用包含一个或多个GPU显卡的单个机器的中等训练和推断系统，再到运行在数百台包含数千个GPU的特制机器的大规模训练系统。拥有一个单一的，可以扩展分布到众多平台上的系统很有意义地简化了真实场景中机器学习系统的使用，正如我们发现，用于大规模训练和小规模部署的单独系统导致大量的维护成本和较差的抽象性。TensorFlow的计算被表示为含状态的数据流图（第2部分详谈），我们专注于使得这个系统既足够灵活，能快速在科研中用新模型来实验，也有充分的高性能和鲁棒性，以应对产品训练和机器学习模型的部署。为了将神经网络训练扩展到更大的部署范围，TensorFlow允许客户通过复制和并行执行一个核心模型数据流图来简单地表达不同的并行，依赖于许多不同的计算设备合作，来更新一个共享的参数集或其他的状态。在描述计算中的适度变化使得可以用很小的代价达到或者尝试很多不同种类的并行方法。TensorFlow的一些用途借助参数更新的一致性来实现灵活性，我们可以在大型部署上轻松表达和利用这些松弛的同步性要求。与DistBelief相比，TensorFlow的编程模式更加灵活，表现明显更好，它支持在更多样化的异构平台上训练和使用更广泛的模型。<br>&emsp;&emsp;大量的DistBelief内部客户已经切换到TensorFlow了。这些客户依靠TensorFlow进行科研和生产，他们的任务多种多样，如在手机上为计算机视觉模型运行推断，到使用大量机器进行带有成千上万个样本记录的成千上万个参数的深度神经网络的大规模训练。尽管这些应用集中于机器学习，尤其是深度神经网络，我们期待TensorFlow的抽象性会在其他不同的领域大显身手，包括其他的机器学习算法以及可能的其他种类的数值计算。我们于2015年11月，在Apache 2.0协议下，将TensorFlow API和一种参考实现开源出来，可以在 <a href="http://www.tensorflow.org" target="_blank" rel="noopener">www.tensorflow.org</a> 上找到。<br>&emsp;&emsp;这篇论文的剩余部分描述了TensorFlow更多的细节。第2部分描述了TensorFlow接口的编程模型和基本概念。第3部分描述了单机和分布式的实现。第4部分描述了一些基本编程模型的扩展。第5部分描述了对基本实现的一些优化。第6部分描述了使用TensorFlow的一些经验。第7部分描述了一些有助于使用TensorFlow的编程习惯用法。第9部分是我们构建在TensorFlow系统核心外围的一些辅助工具。第10部分和第11部分分别讨论了将来和相关的工作。第12部分提供了结论性的想法。</p>
<h1 id="2-编程模型和基本概念"><a href="#2-编程模型和基本概念" class="headerlink" title="2 编程模型和基本概念"></a>2 编程模型和基本概念</h1><p>&emsp;&emsp;TensorFlow的计算由一个有向图描述，这个图由一个节点的集合而成。图代表了一个数据流的计算，并带有一些扩展，这些扩展用于允许一些类型的节点保持并更新一致性状态，以及让图中的分支和循环控制结构具有与Naiad相似的行为方式。客户一般使用一种支持的前端语言（C++或Python）来构建一个计算图。图1中展示了一个在前端使用Python构建并执行一个TensorFlow计算图的例子。图2是这个计算图的结果。<br>&emsp;&emsp;在一个TensorFlow计算图中，每一个节点拥有0个或更多的输入，0个或更多的输出，代表着一个操作的实例化。流过图中正常边（从输出到输入）的值就是张量，即，任意维度的数组，其中的基础元素类型是指定的或者在构造图的过程中推断出来的。图中也存在被称作控制依赖（control dependencies）的特殊边：没有数据流过这类边，但是它们表示，用于控制依赖的源节点必须在用于控制依赖的目的节点开始执行前结束执行。既然我们的模型包括易变的状态，控制依赖可以由客户直接使用，来保证Happens-before关系。同样，我们的实现有时会插入控制依赖来保证独立操作之间的顺序，比如说作为控制内存使用最高峰值的方式。</p>
<h3 id="操作-Operations-和核-Kernels"><a href="#操作-Operations-和核-Kernels" class="headerlink" title="操作(Operations)和核(Kernels)"></a>操作(Operations)和核(Kernels)</h3><p>&emsp;&emsp;一个操作有一个名字，代表着一种抽象的计算（比如，“矩阵乘法”，或“加法”）。一个操作可以有属性，为了实例化一个节点来执行操作，所有的属性必须被提供，或在构造图的时候推断出来。属性通常的一种用法是让操作在不同张量元素类型上多态（例如，两个float类型的张量和两个int32类型的张量）。核是一个操作的特别实现，可以运行在一个特定类型的设备上（如CPU或者GPU）。一个TensorFlow二进制文件通过注册机制定义了操作和可用核集合。这个集合可以通过连接额外的操作/核的定义/注册来加以扩展。表1展示了一些内置进核心TensorFlow二进制文件的操作。</p>
<h3 id="会话-Sessions"><a href="#会话-Sessions" class="headerlink" title="会话(Sessions)"></a>会话(Sessions)</h3><p>&emsp;&emsp;客户端程序通过创建一个会话（Session）来和TensorFlow系统进行交互。为了创建一个计算图，会话（Session）接口支持一种外部的方法来增加目前那些包含额外节点和边的图（当创建一个会话时初始图是空的）。会话接口支持的另一个基本操作是Run，以需要计算的输出名称和替换某些输出节点的张量的操作集合作为输入参数。通过使用Run的参数，TensorFlow的实现可以计算所有节点的必须传递的闭包来计算需要的那些输出，接着会按照可以保证它们依赖的顺序，安排执行合适的节点（在3.1详细讲解）。在TensorFlow的大多数使用中，都是针对一个图设置一个会话，接着执行整个图或者通过上千上万次调用Run来执行一些分离的子图。</p>
<h3 id="变量-Variables"><a href="#变量-Variables" class="headerlink" title="变量(Variables)"></a>变量(Variables)</h3><p>&emsp;&emsp;在大部分计算中，一张图被多次执行。大多数张量（tensor）在执行完一次图后便不再存在。但是，变量（variable）是一种特殊的操作，可以返回指向一个持续可变的张量的一个句柄。这个张量在图的多次执行中仍存活着。指向这些持续可变的张量的句柄可以被传递给一系列特定的操作，诸如Assign和AssignAdd（等同于+=），就可以改变其引用的张量。对于TensorFlow在机器学习中的应用来说，模型的参数典型地存放在张量中，被变量保持着，并作为模型中训练图里Run操作的一部分加以更新。</p>
<h1 id="3-实现"><a href="#3-实现" class="headerlink" title="3 实现"></a>3 实现</h1><p>&emsp;&emsp;TensorFlow系统的主要部件是客户端，它使用会话（Session）的接口来和master，以及一个或多个worker process交流。每个worker process负责仲裁对一个或多个计算设备（CPU核或者GPU显卡）的访问，以及在这些设备上按照master的指示执行图节点。我们既有TensorFlow接口的本地实现，也有分布式实现。本地实现用于客户端，master和worker都运行在单机上的单一操作系统进程（可能包含多个设备，比如，这个机器拥有多个GPU card）。分布式实现与本地实现共享大部分代码，但扩展了本地实现对于客户端，master和worker在不同机器的不同进程中的支持。在我们的分布式环境中，不同的任务就是被簇调度系统分配在job中的容器。这两种模式在图3中进行展示。这部分的剩余大多数内容讨论了两种实现中共有的问题。3.3节塔伦了分布式独有的问题。</p>
<h3 id="设备-Devices"><a href="#设备-Devices" class="headerlink" title="设备(Devices)"></a>设备(Devices)</h3><p>&emsp;&emsp;设备是TensorFlow的计算核心。每个worker负责一个或多个设备，每个设备拥有一个类别和名字。设备名由几部分组成：设备类别标识，在worker中的设备索引，以及在分布式设定中，一个worker的job和任务的标志（或者在设备和进程都在本地时的localhost）。一些设备名的例子如：“/job:localhost/device:cpu:0”，或“/job:worker/task:17/device:gpu:3”。我们对于CPU和GPU都有设备接口的实现，针对其他设备类别的新的设备实现可通过注册机制提供。每一个设备对象负责管理分配和解除分配设备内存，安排由TensorFlow实现中，高一级别对于任意核心的运行请求。</p>
<h3 id="张量-Tensors"><a href="#张量-Tensors" class="headerlink" title="张量(Tensors)"></a>张量(Tensors)</h3><p>&emsp;&emsp;在我们的实现中，一个张量就是一种带类型的多维数组。我们支持若干张量元素类型，包括大小为从8bit到16bit的带符号和无符号整型数，IEEE浮点数和双精度浮点类型，一种复数类型，和一种字符串类型（任意长的字节数组）。合适大小的后台存储通过一个分配器进行管理，这个分配器由该张量所在的设备决定。张量的后台存储缓存是引用计数的，并在没有引用存在时解除分配。</p>
<h2 id="3-1-单设备执行"><a href="#3-1-单设备执行" class="headerlink" title="3.1 单设备执行"></a>3.1 单设备执行</h2><p>&emsp;&emsp;让我们考虑最简单的执行场景：单个设备上的单个worker进程。图的节点按照节点之间的顺序执行。特别地，我们对每个节点做一个计数，记录这个节点上还没有执行的依赖。一旦这个数字减为0，节点就被调度使用，加入一个预备好的队列。这个预备好的队列以一种非指定的顺序被处理，为一个节点指派kernel的执行到设备对象上。当一个节点结束执行后，所有依赖于这个结束节点的计数都会减少。</p>
<h2 id="3-2-多设备执行"><a href="#3-2-多设备执行" class="headerlink" title="3.2 多设备执行"></a>3.2 多设备执行</h2><p>&emsp;&emsp;一旦一个系统具有多设备，有两种主要的复杂情况：决定哪一个设备放置图中每一个节点的计算过程，还有，管理由上一步放置决定造成的设备间的数据通信。这两个问题将在后续部分讨论。</p>
<h3 id="3-2-1-节点放置"><a href="#3-2-1-节点放置" class="headerlink" title="3.2.1 节点放置"></a>3.2.1 节点放置</h3><p>&emsp;&emsp;给出一个计算图，TensorFlow实现的其中一个主要的任务就是将计算映射到可使用的设备集合上。在此给出这个算法的简化版本。这个算法支持的扩展参见4.3节。<br>&emsp;&emsp;放置算法的一个输入是代价模型，包含对于图中每个节点输入和输出张量的尺寸的估计（以字节为单位），以及对于每个节点在输入张量时所需的计算时间的估计。这个代价模型要么是基于关联不同操作类型的启发式规则的静态估计，要么是基于实际的为更早的图的执行而做的置放决定集合衡量。<br>&emsp;&emsp;放置算法首先运行模拟图的执行过程。模拟如下所述进行，最终为图中的每个节点用贪心策略选取一个设备。在这次模拟中，节点到设备的放置过程也被用作真实执行时的放置。<br>&emsp;&emsp;放置算法开始于计算图的源点，并在系统中每个设备上，按照相应的进程模拟活动。对于每个在遍历中到达的节点，可行的设备集合会被考虑到（如果设备没有提供一个实现该特定操作的核，那么此设备或许是不可行的）。对于那些用于多个可选设备的节点，放置算法采用一个贪心启发式规则来考查将节点放置在每一个可能的设备中对于完成时间的影响。这一启发式规则考虑了根据代价模型在那种设备上估计或度量得到的操作时间，也包括了任一用来从其他设备传输输入到该节点的通信的代价。其中最快完成节点操作的设备会被选为该操作的设备，然后放置进程继续针对图中的其他节点进行处理，做出放置决定，包括那些已经准备好自身模拟执行的下游节点。4.3节描述了一些允许用户提供提示和部分限制才指导放置算法的扩展。在此系统中，放置算法仍在持续开发中。</p>
<h3 id="3-2-2-交叉设备通信"><a href="#3-2-2-交叉设备通信" class="headerlink" title="3.2.2 交叉设备通信"></a>3.2.2 交叉设备通信</h3><p>&emsp;&emsp;一旦计算好节点放置，图就被划分成子图的集合，一张子图对应一个设备。从x到y任何交叉设备的边会被移除，并被一条在x的子图中从x到一个新的Send节点的边，以及一条在y的子图中，从对应的Receive节点到y的边代替。图4是这种图变换的一个例子。<br>&emsp;&emsp;在运行时刻，Send节点的实现和Receive节点的实现合作进行跨设备的数据交换。这一点使得我们可以在Send和Receive的内部实现中，隔离所有的通信，简化了运行时刻剩余部分的工作。<br>&emsp;&emsp;当我们插入Send和Receive节点时，我们将在一特定设备上一特定张量的所有使用者加以规范化，来使用单个的Receive节点，而不是对特定设备上每个下游使用者都给一个Receive节点。这样可以确保对于所需的张量数据只会在一个源设备→目的设备之间传递一次，在目的设备上，对于张量的内存也只会被分配一次，而非多次（例如，参见图4中的b和c节点）。<br>&emsp;&emsp;通过这种方法处理通信，我们也允许在不同设备上的图的个别节点调度被去中心化到worker上：Send和Receive节点在不同的worker和设备间传递必要的同步信息，master只需要针对每个图的执行，给每一个拥有图中任意节点的worker给出一个单独的Run请求。这样使得系统更加可扩展，使得执行的节点比起用户强制调度，拥有更加精细的粒度。</p>
<h2 id="3-3-分布式执行"><a href="#3-3-分布式执行" class="headerlink" title="3.3 分布式执行"></a>3.3 分布式执行</h2><p>&emsp;&emsp;图的分布式执行和多设备执行很相似。在放置设备之后，针对每个设备会创建一张子图。跨worker进程通信的Send/Receive节点对使用诸如TCP或者RDMA的远程通信机制机器间移动数据。</p>
<h3 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h3><p>&emsp;&emsp;在分布式执行中，错误可以在不同的地方被探查到。主要的有（a）在一个Send和Receive节点对之间的通信错误，和（b）从master进程到每个worker进程的周期性健康检查。<br>&emsp;&emsp;当一个错误被发现后，整个图的执行会中止，并从头开始。但是会重新调用那个变量(Variable)节点，此节点引用了图的执行过程中保持的张量。在重启过程中，我们支持一致的检查点和该状态的恢复。尤其是，每一个变量节点连接在一个Save节点之上。这些Save节点周期性执行，比如说，每N次迭代执行一次，或每N秒执行一次。当它们执行的时候，变量的内容被写入持久化的存储之中，例如，一个分布式文件系统。类似的，每一个变量连接到一个Restore节点，此节点只会在一次重启之后的第一次迭代中启用。关于某些节点只能在图的某些执行中才能启用的细节，参见4.2节。</p>
<h1 id="4-扩展"><a href="#4-扩展" class="headerlink" title="4 扩展"></a>4 扩展</h1><p>&emsp;&emsp;本节中，我们介绍第2节引入的基本编程模型的几种更高阶的特性。</p>
<h2 id="4-1-梯度计算-Gradient-Computation"><a href="#4-1-梯度计算-Gradient-Computation" class="headerlink" title="4.1 梯度计算(Gradient Computation)"></a>4.1 梯度计算(Gradient Computation)</h2><p>&emsp;&emsp;许多优化算法，包括像随机梯度下降（stochastic gradient descent）这样的常见机器学习训练算法，计算一个代价函数关于一个输入集的梯度。因为这是一个如此常见的需求，所以TensorFlow对于自动梯度计算有内建的支持。如果在TensorFlow图中，一个张量C，或许通过一个复杂的操作子图，依赖于某个张量集合{Xk}，那么存在一个内建的函数将会返回张量{dC/dXk}。和其他张量一样，梯度张量是采用以下步骤，通过扩展TensorFlow的图被计算出来的。<br>&emsp;&emsp;当TensorFlow需要在C所依赖的上面，计算一个张量C关于某个张量I的梯度时，首先在计算图中找到从I到C的路径。接着回溯从C到I，并对在回溯路径上每一个操作，它都往TensorFlow的图中加入一个节点，包括沿着回溯路径使用链式法则的偏导数。新加入的节点计算前向路径中对应操作的梯度函数。梯度函数可以是任何的操作。这个函数不但可以用沿着回溯路径计算出的偏导数作为输入，也可以选择性地用前向操作的输入输出作为输入。图5展示了从图2示例计算得到的一个代价函数的梯度。灰色箭头表示梯度函数可能的输入。这些梯度函数并不是展示出的特定操作使用的。图1所对应的是计算这些梯度：<br>&emsp;&emsp;[db,dW,dx] = tf.gradients(C, [b,W,x])<br>&emsp;&emsp;总体而言，一个操作或许有多个输出，张量C也许依赖于其中的某几个输出。举个例子，如果操作O拥有两个输出y1和y2，C只依赖于y2，既然 dC/dy1 = 0，那么O的梯度函数的第一个输入就设为0。<br>&emsp;&emsp;自动梯度计算使得优化，尤其是内存的使用变得复杂。当执行“前向”计算子图时，即，那些由用户明确指示的子图，在通过观察构造图的顺序来决定接下来执行哪个节点时，一种明智的启发式方法会断开关系。这大致意味着临时的输出会在构建不久之后就会用到，所以它们的内存可以很快被重复使用。当该启发式方法不再有效时，用户可以改变图构造的顺序，或者如图5所述，加入控制依赖。当梯度节点被自动加入图中时，用户控制会减弱，启发式方法会失效。特别地，因为梯度逆转了前向计算的顺序，在梯度计算临近结束的时候，图中先前被使用的张量再次被频繁需要。这样的张量消耗很多GPU内存，也就不必要地限制了计算的规模。我们正在积极地提升内存管理来更好地处理这样的情况。选项包括使用更加复杂的启发式方法来决定图的执行顺序，重新计算张量而不是将其保存在内存中，以及将长期张量从GPU内存移动到更大的CPU内存中。</p>
<h2 id="4-2-部分执行"><a href="#4-2-部分执行" class="headerlink" title="4.2 部分执行"></a>4.2 部分执行</h2><p>&emsp;&emsp;一个客户端经常会想要执行完整执行图的一个子图。为了支持这一点，一旦该客户端在一个Session中设置了一个计算图，我们的Run方法允许他们执行整个图中一张任意的子图，沿着图中任意边注入任意数据，以及获取流经图中任意边的数据流。<br>&emsp;&emsp;图中的每个节点都有一个名字，每个节点的每个输出由源节点的名字和节点的输出端口定义，从0开始计数（例如，“bar:0”代表“bar”节点的第1个输出，“bar:1”代表第2个输出）。<br>&emsp;&emsp;Run调用的两个参数可以有助于定义准确的执行在即的子图。首先，Run调用接受输入，一个可选的映射name:port填充“fed”张量的值。第二，Run调用接受output_names，一个规范指明了哪个节点该被执行的name[:port]输出列表，并且，如果端口(port)部分以名字出现，那么在Run调用成功的情况下，对那个节点的特定输出张量的值应该返回到客户端。<br>&emsp;&emsp;图是基于输入和输出的值进行变换的。每一个输入指定的node:port被一个feed节点代替，该节点会在Run调用所用的Rendezvous对象里，从特定初始化的入口中选取被提供的输入张量。类似的，每一个带有端口的输出名与一个特殊的fetch节点相连，可供输出张量的存储，并在Run调用完成之时返回到客户端。最后，一旦图被特定的feed和fetch节点的插入重写，可以通过从任何输出命名的每个节点开始并使用图形依赖关系在图形中向后工作来确定要执行的节点集合，以确定必须在重写的图形中执行的完整节点集合以便计算输出。图6展示了左边的原始图，以及用inputs=={b}和outputs=={f:0}激活Run调用产生的变换图。既然我们只需要计算f节点的输出，我们将不执行d和e节点，它们对f的输出毫无贡献。</p>
<h2 id="4-3-设备限制"><a href="#4-3-设备限制" class="headerlink" title="4.3 设备限制"></a>4.3 设备限制</h2><p>&emsp;&emsp;TensorFlow的客户端可以通过提供对于一个节点可以执行在哪台设备上的部分限制，来控制设备上节点的放置。例如，“只能把这个节点放置在GPU类型的设备上”，或者，“这个节点可以被放在/job:worker/task:17的任何设备上”，或者，“这个节点和名叫variable13的节点共享位置”。在这些限制的边界之内，放置算法负责选择一种分配节点到设备上去的方式，该算法提供计算的快速执行，也满足由这些设备自己的不同限制，比如说，为了执行图节点的子集而在设备上限制需要的内存总数。<br>&emsp;&emsp;支持这样的限制需要改变3.2.1节所说的放置算法。我们首先计算每个节点可用的设备集合，接着使用图中那些共享位置的限制的并查集计算出必须被放置在一起的图的部件。对于每一个这样的部件，我们计算可用设备的交集。计算得到的每个节点的可行设备集和放置算法的模拟器很容易适配。</p>
<h2 id="4-4-控制流程"><a href="#4-4-控制流程" class="headerlink" title="4.4 控制流程"></a>4.4 控制流程</h2><p>&emsp;&emsp;尽管不带任何显式控制流程的数据流图相当强大，我们还是注意到在许多案例中，支持条件和循环可以得到更加简洁而有效的机器学习算法表示。<br>&emsp;&emsp;正如Arvind所述的数据流型机器的方法一样，我们在TensorFlow中引入原始控制流操作的一个小集合，并将TensorFlow推广到处理带周期的数据流图。Switch和Merge操作允许我们跳过执行一个完整的子图。这个完整子图基于一个布尔值张量的值。Enter，Leave和NextIteration操作允许我们表示迭代。诸如if条件和while循环这样的高阶编程构造可以用这些控制流操作被很简单地编译进数据流图。<br>&emsp;&emsp;TensorFlow的运行实现了一个tags和frames的概念，在概念上与MIT Tagged-Token machine类似。一次循环的每一次迭代由一个tag唯一定义，它的执行状态由一个frame表示。只要变为可用，一个输入就可以在任何时候进入一次迭代；所以说，多次迭代可以被准确地执行。<br>&emsp;&emsp;TensorFlow使用一种分布式协同机制来用控制流执行图。大体而言，一次循环可以包含那些被分配到许多不同设备的节点。因此，管理一次循环的状态成为分布式终端探查的一个问题。TensorFlow的解决方案是基于图的重写。在图被划分的期间，我们自动在每个划分中加入控制节点。这些节点实现一个每次迭代中用于编排开始和终末的小型状态机，并决定循环的终止。对于每一次迭代，拥有循环终止谓词的设备向每个参与设备发送微小的控制消息。<br>&emsp;&emsp;正如上述所言，我们经常通过梯度下降来训练机器学习模型，并把计算梯度表示为数据流图的一部分。当一个模型包括控制流操作时，我们必须在对应的梯度计算中对此做个计数。例如，带有if条件的模型的梯度计算需要指定选择了条件的哪个分支，接着对该分支应用梯度逻辑。类似的，对于带有while循环的模型的梯度计算需要知道共有多少次迭代，也依赖于这些迭代中出现的中间值。基本的技术是重写图，这样才能记住梯度计算需要的值。我们省略了这种编码的一些内容细节。</p>
<h2 id="4-5-输入操作"><a href="#4-5-输入操作" class="headerlink" title="4.5 输入操作"></a>4.5 输入操作</h2><p>&emsp;&emsp;尽管可以通过feed节点把输入数据提供给计算，另一种用来训练大规模机器学习模型的常见机制是在图中采用特定的输入操作节点。这些节点一般通过文件名配置，并从每次执行时的那些文件集合中存放的数据中产生一个包含一个或者多个样本的张量。这使得数据可以被直接从底层存储系统中读取出来，到将要执行接下来处理数据的机器的内存中。在客户端进程和worker进程分离开的配置中，如果数据被给出，它一般会需要一个额外的网络hop（从存储系统到客户端，接着从客户端到worker vs 直接从存储系统到正在使用一个输入节点的worker）。</p>
<h2 id="4-6-队列"><a href="#4-6-队列" class="headerlink" title="4.6 队列"></a>4.6 队列</h2><p>&emsp;&emsp;队列是我们加到TensorFlow中的一种有用的特性。它们使得图的不同部分执行异步，可能按照不同的节奏，通过Enqueue和Dequeue操作处理数据。Enqueue操作会被阻塞，直到队列中存在可用空间。Dequeue操作会被阻塞，直到在队列中所要求的最小数量的元素可用。队列的一种用处就是允许输入数据可以从磁盘文件中预先取出，与此同时，机器学习模型的计算部分可以处理前一个批次的数据。它们也可以被用于其他类型的归类，包括聚合很多梯度来在更大的批次上计算某种更复杂的梯度集合，或者把不同的输入语句组织成递归的语言模型到语句的长度一致的bins中。这样做可以处理地更有效率。<br>&emsp;&emsp;除了正常的FIFO队列，我们也实现了一个shuffling队列。这种队列可以对内存中一大块缓冲区的元素进行随机洗牌。这种随机洗牌的功能对于那些想要随机化样本顺序的机器学习算法很有用。</p>
<h2 id="4-7-容器"><a href="#4-7-容器" class="headerlink" title="4.7 容器"></a>4.7 容器</h2><p>&emsp;&emsp;容器是TensorFlow内部的一种机制，用于管理长期存在的可变状态。变量（Variable）的后备存储位于容器中。默认的容器一直存在，直到进程终止，但我们也允许其他已被命名的容器。一个容器可以通过清除其整个内容进行重置。使用容器，就可以沿着完全不相交的计算图，和不同的会话分享状态。</p>
<h1 id="5-优化"><a href="#5-优化" class="headerlink" title="5 优化"></a>5 优化</h1><p>&emsp;&emsp;在这一节中，我们讨论TensorFlow实现内部的优化。这些优化对于系统表现或者资源使用起到了提升作用。</p>
<h2 id="5-1-共同子表达式消除"><a href="#5-1-共同子表达式消除" class="headerlink" title="5.1 共同子表达式消除"></a>5.1 共同子表达式消除</h2><p>&emsp;&emsp;既然计算图的构建经常是由客户端代码中的不同层的抽象完成的，那么计算图就很容易产生同一个计算的冗余拷贝。为了解决这个问题，我们实现了一个公共的子表达式传递，类似于Click给出的算法，该算法运行于计算图上，对这些节点中的单独一个节点，将操作的多个拷贝用相同的输入和操作类型加以规范化，并且准确地重定向图的边来反映这种规范化。</p>
<h2 id="5-2-控制数据通信和内存分配"><a href="#5-2-控制数据通信和内存分配" class="headerlink" title="5.2 控制数据通信和内存分配"></a>5.2 控制数据通信和内存分配</h2><p>&emsp;&emsp;精心规划的TensorFlow操作可以得到更好的结果。特别是在数据传输和内存使用方面，特别地，调度可以减少时间窗口，在此期间，中间结果需要在操作之间保留在内存中，从而减少峰值内存消耗。对于内存稀缺的GPU设备来说，这样的减少尤其重要。此外，管理跨设备的数据通信可以减少对网络资源的争用。<br>&emsp;&emsp;虽然有很多机会来调度优化，但我们在此专注于我们认为特别必要和有效的事情。它涉及用于读取远程值的Receive节点的调度。如果不采取预防措施，这些节点可能比必要时更早开始，可能在执行开始时一次性完成。作为操作研究的一种常见情况，通过执行as-soon-as-possible/as-late-as-possible(ASAP/ALAP)计算，我们分析了图的关键路径，以估计出何时启动Receive节点。接着，为了拖延这些节点的启动，直到需要它们的这些结果，我们插入控制的边。</p>
<h2 id="5-3-异步核-kernel"><a href="#5-3-异步核-kernel" class="headerlink" title="5.3 异步核(kernel)"></a>5.3 异步核(kernel)</h2><p>&emsp;&emsp;除了在计算方法的最后完成执行的正常异步核之外，我们的框架还支持非阻塞内核。这种非阻塞内核使用一种略微不同的接口，因此Compute方法被传递一个应该在内核执行完成时调用的延续。这是针对具有许多活动线程的环境在内存使用或其他资源方面相对昂贵的环境的优化，并允许我们在等待I/O或其他事件发生时避免在无限期内绑定执行线程。异步内核的例子包括Receive内核，以及Enqueue和Dequeue内核（如果队列空间不可用或者没有数据可分别读取，则可能需要阻塞）。</p>
<h2 id="5-4-用于核-kernel-实现的优化库"><a href="#5-4-用于核-kernel-实现的优化库" class="headerlink" title="5.4 用于核(kernel)实现的优化库"></a>5.4 用于核(kernel)实现的优化库</h2><p>&emsp;&emsp;我们经常使用预先存在的高度优化的数学库来实现用于一些操作的核。例如，有很多优化库用于在不同设备上执行矩阵乘法，包括BLAS和cuBLAS，或用于深度神经网络卷积内核的GPU库，比如cuda-convnet和cuDNN。我们的许多内核实现都是围绕此类优化库的相对较薄的包装器。<br>&emsp;&emsp;我们相当广泛地运用了开源的Eigen线性代数库，用于系统中许多内核的实现。作为TensorFlow开发的一部分，我们的团队（主要是Benoit Steiner）通过支持任意维度张量操作扩展了开源Eigen库。</p>
<h2 id="5-5-有损压缩"><a href="#5-5-有损压缩" class="headerlink" title="5.5 有损压缩"></a>5.5 有损压缩</h2><p>&emsp;&emsp;有些机器学习算法，包括那些典型地用来训练神经网络的机器学习算法，容忍噪声和降低精度的运算。以一种类似于DistBelief系统的方式，当在设备间发生数据时（有时候是在同一个机器内部但特别是沿着机器边界），我们经常使用高精度内部表示的有损压缩。例如，我们经常插入特殊的转换节点，将32位浮点数转换成16位浮点表示（不是提议的IEEE16位浮点数标准，而是一种32位IEEE794浮点格式，但在尾数处精度低16位），接着在通信信道的另一端转换回32位表示（通过为尾数的缺失部分补0，因为这样做计算成本更低，比起在进行32→16→32位转换时，在数学上做正确的概率舍入。）</p>
<h1 id="6-状态和经验"><a href="#6-状态和经验" class="headerlink" title="6 状态和经验"></a>6 状态和经验</h1><p>&emsp;&emsp;TensorFlow接口和一个参考实现已经在Apache 2.0许可下开源，系统可在 <a href="http://www.tensorflow.org" target="_blank" rel="noopener">www.tensorflow.org</a> 下载。此系统包括详细的文档，许多教程和大量例程，演示如何针对不同的机器学习任务使用系统。例程包括从MNIST数据集（相当于机器学习算法的“hello world”）分类手写数字，从CIFAR-10数据集分类图像，使用循环的LSTM网络进行语言建模，训练词嵌入量等模型。<br>&emsp;&emsp;此系统包括在Python和C++中指定TensorFlow计算的前端，我们希望随着时间的推移能增加其他的前端，以回应Google内部和更广泛的开源社区的用户的要求。<br>&emsp;&emsp;在我们先前的DistBelief系统中已经有很多机器学习模型了，我们将其迁移到了TensorFlow。本节的剩余部分讨论我们学到的一些经验教训，这些经验教训对于任何将机器学习模型从一个系统迁移到另一个系统的过程都很适用，因此对其他人很有价值。<br>&emsp;&emsp;特别地，我们重点关注将最先进的卷积神经网络移植到名为Inception的图像识别中获取的经验教训。该图像识别系统将224x224像素的图像分类成1000个标签中的一个（例如，“猎豹”，“垃圾车”，等等）。当用TensorFlow图表示的时候，这样一个模型包含1360万张可学习的的参数和36000个操作。在单个图像上运行推理需要20亿次乘加运算。<br>&emsp;&emsp;在TensorFlow中建立所有必要的数学运算之后，将所有36,000个操作组装和调试到正确的图形结构中十分具有挑战性。验证正确性是一项困难的事情，因为系统本质上是随机的，并且只能在预期中以某种方式表现——可能在经过数小时的计算之后。考虑到这些情况，我们发现下述策略对于将Inception模型移植到TensorFlow中至关重要。<br>&emsp;&emsp;1.构造工具以深入了解一个给定模型参数中的准确数量。这样的工具在一个复杂的网络体系结构中显示出微妙的缺陷。尤其是我们能够识别由于在一个维度上的数学运算中的自动广播而不正确地实例化的操作和变量。<br>&emsp;&emsp;2.从小处着手再扩大规模。我们从先前系统中移植的第一个卷积神经网络是一个部署在CIFAR-10数据集上的微型网络。调试这样的一个网络阐述了机器学习系统中单个操作的微妙边界情况（例如，最大池化），而这样的网络在更加复杂的网络里事实上难以解读。<br>&emsp;&emsp;3.在学习关闭时，始终确保机器学习系统之间的目标（损失函数）匹配。设置学习率为0有助于我们识别模型中随机初始化的变量的意外行为。这样一个错误在动态训练网络中是很难识别出来的。<br>&emsp;&emsp;4.在调试一个分布式实现前，先做一个单机的实现匹配。此策略帮助我们勾画和调试在机器学习系统之间训练表现的差异。尤其是，我们确认出bug的原因是竞态条件和被错认为原子操作的非原子操作。<br>&emsp;&emsp;5.防范数字错误。数值库在处理非有限浮点值时并不一致。卷积神经网络对于数值的不稳定特别敏感，并在实验和调试阶段往往会带规律地发散。通过检查非有限浮点值来防止出现此种行为，使得我们实时发现错误，而不是事后的发散行为。<br>&emsp;&emsp;6.分析一个网络的各个部分并理解数值错误的大小。在两个机器学习系统上并行运行神经网络的子设备提供了一种精确的方法，以确保两个系统中的数值算法是相同的。鉴于这些算法以浮点精度运行，重要的是预测和理解预期的数值误差的大小，以判断给定的组件是否正确实现（例如，区分“在1e-2内，好！”和“在1e-2内：为什么是错的？！”）。<br>在存在固有随机系统的情况下验证复杂的数学运算是非常具有挑战性的。上述策略在获得对系统的信心以及最终在TensorFlow中实现Inception模型方面证明是非常宝贵的。与我们现有的模型的DistBelief实现相比，这些努力的最终结果使训练时间提高了6倍，并且这种速度增益在训练一类新的大型图像识别模型中证明是不可或缺的。</p>
<h1 id="7-常用编程规范"><a href="#7-常用编程规范" class="headerlink" title="7 常用编程规范"></a>7 常用编程规范</h1><p>&emsp;&emsp;TensorFlow的基本数据流图模型可以以多种方式应用在机器学习应用中。我们关心的一个领域是加速大型数据集上计算密集型神经网络模型的训练。本节介绍了我们和其他人为了达到该目的而开发的几种技术，展示了如何使用TensorFlow来实现这些不同的方法。<br>&emsp;&emsp;本小节中的方法假设使用随机梯度下降（SGD）训练模型，其具有相对适中大小的100至1000个示例的小批量。</p>
<h2 id="数据并行训练"><a href="#数据并行训练" class="headerlink" title="数据并行训练"></a>数据并行训练</h2><p>&emsp;&emsp;一个加速SGD的简单方法就是在小批量元素中并行进行小批量梯度的计算。例如，如果我们使用一个1000个元素的小批量大小，我们可以使用模型的10个副本，每个副本计算100个元素的梯度。接着合并梯度并同步更新参数，</p>
<h1 id="8-性能"><a href="#8-性能" class="headerlink" title="8 性能"></a>8 性能</h1><p>&emsp;&emsp;本白皮书的未来版本将包含单机和分布式实现的综合性能评估部分。</p>
<h1 id="9-工具"><a href="#9-工具" class="headerlink" title="9 工具"></a>9 工具</h1><h2 id="9-1-TensorBoard：图结构和总结统计的可视化"><a href="#9-1-TensorBoard：图结构和总结统计的可视化" class="headerlink" title="9.1 TensorBoard：图结构和总结统计的可视化"></a>9.1 TensorBoard：图结构和总结统计的可视化</h2><h2 id="计算图的可视化"><a href="#计算图的可视化" class="headerlink" title="计算图的可视化"></a>计算图的可视化</h2><h2 id="汇总数据的可视化"><a href="#汇总数据的可视化" class="headerlink" title="汇总数据的可视化"></a>汇总数据的可视化</h2><h2 id="9-2-性能跟踪"><a href="#9-2-性能跟踪" class="headerlink" title="9.2 性能跟踪"></a>9.2 性能跟踪</h2><h1 id="10-未来工作"><a href="#10-未来工作" class="headerlink" title="10 未来工作"></a>10 未来工作</h1><p>&emsp;&emsp;我们对于未来的工作有几种不同的方向。我们将继续使用TensorFlow来为人工智能开发新的有趣的机器学习模型。在这个过程中，我们可能会发现我们需要扩展基本的TensorFlow系统。开源社区也会针对TensorFlow的实现产生新的有趣的指导。<br>&emsp;&emsp;在我们的考虑中，一个对于基本编程模型的扩展就是函数机制，用户可以将TensorFlow计算的一整张子图指定为可重用部件。在我们设计的实现中，这些函数可以成为TensorFlow的跨越不同前端语言的可重用部件，这样一来，用户可以使用Python前端语言定义一个函数，接着将此函数用来做C++前端的基本构建块。我们很希望这种跨语言的重用能够引导一个充满活力的机器学习研究人员社区。在这社区里，他们不仅仅发布他们研究的完整范例，也有来自他们工作中的，可以在其他环境中重用的小部件。<br>&emsp;&emsp;我们也有许多具体的方向来提高TensorFlow的性能。其中一种方向便是我们在即时编译器上的初始工作。这个即时编译器可以获取TensorFlow计算的一个子图，或许还带着关于张量典型尺寸和形状的一些运行时的分析信息，并可以为为该子图生成优化的例程。该编译器理解许多优化操作的语义，例如，循环融合，局部区域的阻塞和平铺，特定形状和大小的专门化等等。<br>&emsp;&emsp;我们还设想了一个有意义的未来工作领域，那就是改进放置和节点调度算法，用于决定哪里的不同节点将会执行，以及它们应该何时开始执行。我们目前在子系统上实现了一些启发式算法，而且我们想要让系统学会做出良好的放置决定（或许使用一种深度神经网络，并结合强化学习目标函数）。</p>
<h1 id="11-相关工作"><a href="#11-相关工作" class="headerlink" title="11 相关工作"></a>11 相关工作</h1><p>&emsp;&emsp;还有许多其他的系统都可以用很多方式和TensorFlow比较。Theano，Torch，Caffe，Chainer和计算网络工具包都是一些主要设计用来训练神经网络的系统。与分布式的TensorFlow实现不同，这些系统中的每一个都把计算映射到单个的机器上。像Theano和Chainer一样，TensorFlow支持象征性的分化，所以说更容易定义和使用基于梯度的优化算法。和Caffe一样，TensorFlow有一个C++写成的core，简化了在大量广泛的产品设置上部署训练模型，包括移动设备这种带有内存限制和计算限制的环境。<br>&emsp;&emsp;TensorFlow系统和它前身DistBelief共享一些设计元素，并与像Project Adam和Parameter Server项目这样的后继系统有着相似的设计。和DistBelief和Project Adam一样，TensorFlow允许计算跨机器跨计算设备分散，并可以让用户使用相对高阶的描述指定机器学习模型。然而，与DistBelief和Project Adam不一样，在TensorFlow中的通用数据流图更加灵活，更适合表达一种更多种类的机器学习模型和优化算法。它也允许将有状态参数节点的表达式作为变量进行显著的简化，变量更新的操作是图中的附加节点。与此相反，DistBelief，Project Adam和 Parameter Server系统全都有完整而独立的，致力于通信和更新参数值的参数服务器子系统。<br>&emsp;&emsp;用于表示图像处理流水线的Halide系统，使用一种和TensorFlow数据流图相似的中间表示。然而，和TensorFlow不同的是，Halide系统事实上对其操作的语义拥有更高阶的知识。考虑到并行性和局部性，Halide使用这知识生成高度优化的，结合了多个操作的代码段。Halide只在单个机器上运行计算结果，而非分布式设置。在未来的工作中，我们希望使用类似的跨操作动态编译框架来扩展TensorFlow。<br>&emsp;&emsp;和TensorFlow一样，一些其他的分布式系统也已经被开发出来用于跨集群执行数据流图。Dryad和Flume演示了如何把一个复杂的工作流程表示为一个数据流图。CIEL和Naiad引入了对于数据依赖型控制流程的通用支持：CIEL表示作为一个DAG动态展开的迭代，而Naiad使用带周期的静态图来支持低延迟的迭代。Spark使用“弹性分布数据集”（RDDS），被优化用于计算重复相同的数据。RDDS是早期计算的软态缓存输出。Dandelion在包括GPU的异构设备集群之间执行数据流图。TensorFlow从上述这些系统中借用元素，使用一种混合数据流模型。它的数据流调度器——选择下一个执行节点的部件，使用与Dryad, Flume, CIEL和Spark一样的基本算法。它的分布式架构接近于Naiad，因为这个系统使用一种单独的，被优化过的数据流图来表示整个计算，并在每一台设备上缓存图的信息，来最小化减少合作开销。和Spark，Naiad类似，当集群中存在足够的RAM来保存计算的工作集时，TensorFlow的表现最好。TensorFlow中的迭代使用混合方法：同一个数据流图的多个副本可以一次执行，并共享同一个变量集。副本可以通过变量异步共享数据，或者在图里使用同步机制，如队列，进行同步操作。TensorFlow海支持图中的迭代，这是一种CIEL和Naiad的混合：为简单起见，每一个节点只有当它的所有输入都准备就绪时才会触发（就像CIEL）；但出于效率起见，图被表示为一种静态的循环数据流（就像Naiad）。</p>
<h1 id="12-结论"><a href="#12-结论" class="headerlink" title="12 结论"></a>12 结论</h1><p>&emsp;&emsp;我们设计出了TensorFlow——一个灵活的，基于数据流的编程模型，以及此编程模型的单机和分布式实现。此系统源自在实施科学研究以及在各种Google产品和服务上部署超过一百个机器学习项目的实际经验。我们开源了TensorFlow的一个版本，并希望一个活跃的共享社区围绕着TensorFlow的使用发展起来。我们很高兴看到Google以外的其他人在他们的工作中使用TensorFlow。</p>
<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>&emsp;&emsp;TensorFlow的发展大大受益于Google大量而广泛的机器学习社区，尤其是来自Google Brain团队其他成员，以及来自Google内部数百个DistBelief和TensorFlow的用户的建议和贡献。毫无疑问，通过聆听他们的反馈，TensorFlow的可用性和功能性得到了巨大的提升。<br>&emsp;&emsp;许多人为TensorFlow及其开源版本做出了贡献，其中包括John Giannandrea（创建支持性研究环境），Irina Kofman和Phing Turner（项目管理），Bill Gruber和David Westbrook（技术写作），Dave Andersen，Anelia Angelova ，Yaroslav Bulatov，Jianmin Chen，Jerjou Cheng，George Dahl，Andrew Dai，Lucy Gao，mig Gerard，Stephan Gouws，Naveen Kumar，Geoffrey Hinton，Mrinal Kalarishnan，Anjuli Kannan，Yutaka Leon-Suematsu，Frank Li，Peter Liu，刘小兵，Nishant Patil，Pierre Sermanet，Noam Shazeer，Jascha Sohl-dickstein，Philip Tucker，Yonghui Wu，Ke Yang和Cliff Young（总贡献），Doug Fritz，Patrick Hurst，Dilip Krishnan，Daniel Smilkov，James Wexler，Jimbo Wilson， Kanit Ham Wongsuphasawat，Cassandra Xia和Big Picture团队（图表可视化），Chris Leary，Robert Springer和Stream Executor团队，Kayur Patel，Michael Piatek和coLab团队以及许多其他为TensorFlow设计和代码库做出贡献的人。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-mawat, Ian Goodfellow, Andrew Harp, Geoffrey Irv-ing, Michael Isard, Yangqing Jia, Rafal Jozefowicz,Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner,Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.<br>[2] Anelia Angelova, Alex Krizhevsky, and Vincent Vanhoucke. Pedestrian detection with a large-field-of-view deep network. In Robotics and Automation (ICRA), 2015 IEEEInternationalConferenceon, pages704–711.IEEE, 2015. CalTech PDF.<br>[3] Arvind and David E. Culler. Annual review of computer science vol. 1, 1986. chapter Dataflow Architectures, pages 225–253. 1986. <a href="http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2&amp;doc=GetTRDoc.pdf&amp;AD=ADA166235" target="_blank" rel="noopener">www.dtic.mil/cgi-bin/GetTRDoc?Location=U2&amp;doc=GetTRDoc.pdf&amp;AD=ADA166235</a>.<br>[4] Arvind and Rishiyur S. Nikhil. Executing a program on the MIT tagged-token dataflow architecture. IEEE Trans. Comput.,39(3):300–318, 1990.dl.acm.org/citation.cfm?id=78583.<br>[5] Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu.Multiple object recognition with visual attention. arXiv preprint arXiv:1412.7755, 2014.arxiv.org/abs/1412.7755.<br>[6] Franc ¸oise Beaufays. The neural networks behind Google Voice transcription, 2015. googleresearch.blogspot.com/2015/08/the-neural-networks-behind-google-voice.html.<br>[7] James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A CPU and GPU math expression compiler. In Proceedings of the Python for scientific computing conference (SciPy), volume 4, page 3. Austin, TX, 2010. UMontreal PDF.<br>[8] Craig Chambers, Ashish Raniwala, Frances Perry, Stephen Adams, Robert R Henry, Robert Bradshaw, and Nathan Weizenbaum. FlumeJava: easy, efficient data-parallel pipelines. In ACM Sigplan Notices, volume 45, pages 363–375. ACM, 2010. research.google.com/pubs/archive/35650.pdf.<br>[9] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. cuDNN: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759, 2014. arxiv.org/abs/1410.0759.<br>[10] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project Adam: Building an efficient and scalable deep learning training system. In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), pages 571–582, 2014. <a href="http://www.usenix.org/system/files/conference/osdi14/osdi14-paper-chilimbi.pdf" target="_blank" rel="noopener">www.usenix.org/system/files/conference/osdi14/osdi14-paper-chilimbi.pdf</a>.<br>[11] Jack Clark. Google turning its lucrative web search over to AI machines, 2015. <a href="http://www.bloomberg.com/news/articles/2015-10-26/google-turning-its-lucrative-web-search-over-to-ai-machines" target="_blank" rel="noopener">www.bloomberg.com/news/articles/2015-10-26/google-turning-its-lucrative-web-search-over-to-ai-machines</a>.<br>[12] Cliff Click. Global code motion/global value numbering. In ACM SIGPLAN Notices, volume 30, pages 246–257. ACM, 1995. courses.cs.washington.edu/courses/cse501/06wi/reading/click-pldi95.pdf.<br>[13] Ronan Collobert, Samy Bengio, and Johnny Mariéthoz. Torch: A modular machine learning software library. Technical report, IDIAP, 2002.infoscience.epfl.ch/record/82802/files/rr02-46.pdf.<br>[14] Jeffrey Dean, Gregory S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker,Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In NIPS, 2012. Google Research PDF.<br>[15] Jack J Dongarra, Jeremy Du Croz, Sven Hammarling, and Iain S Duff. A set of level 3 basic linear algebra subprograms. ACM Transactions on Mathematical Software (TOMS), 16(1):1–17, 1990. <a href="http://www.maths.manchester.ac.uk/˜sven/pubs/Level3BLAS-1-TOMS16-90.pdf" target="_blank" rel="noopener">www.maths.manchester.ac.uk/˜sven/pubs/Level3BLAS-1-TOMS16-90.pdf</a>.<br>[16] Andrea Frome, Greg S Corrado, Jonathon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. DeVISE: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121–2129, 2013. research.google.com/pubs/archive/41473.pdf.<br>[17] Javier Gonzalez-Dominguez, Ignacio Lopez-Moreno, Pedro J Moreno, and Joaquin Gonzalez-Rodriguez. Frame-by-frame language identification in short utterances using deep neural networks. Neural Networks, 64:49–58, 2015.<br>[18] Otavio Good. How Google Translate squeezes deep learning onto a phone, 2015.googleresearch.blogspot.com/2015/07/how-google-translate-squeezes-deep.html.<br>[19] Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit number recognition from Street View imagery using deep convolutional neural networks. In International Conference on Learning Representations, 2014. arxiv.org/pdf/1312.6082.<br>[20] Georg Heigold, Vincent Vanhoucke, Alan Senior, Patrick Nguyen, Marc’Aurelio Ranzato, Matthieu Devin, and Jeffrey Dean. Multilingual acoustic models using distributed deep neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8619–8623. IEEE, 2013. research.google.com/pubs/archive/40807.pdf.<br>[21] Geoffrey E. Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen，Tara N. Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Process. Mag., 29(6):82–97, 2012. <a href="http://www.cs.toronto.edu/˜gdahl/papers/deepSpeechReviewSPM2012.pdf" target="_blank" rel="noopener">www.cs.toronto.edu/˜gdahl/papers/deepSpeechReviewSPM2012.pdf</a>.<br>[22] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. ftp.idsia.ch/pub/juergen/lstm.pdf.<br>[23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. arxiv.org/abs/1502.03167.<br>[24] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly. Dryad: distributed data-parallel programs from sequential building blocks. In ACM SIGOPS Operating Systems Review, volume 41, pages 59–72. ACM, 2007. <a href="http://www.michaelisard.com/pubs/eurosys07.pdf" target="_blank" rel="noopener">www.michaelisard.com/pubs/eurosys07.pdf</a>.<br>[25] Benoˆ ıt Jacob, Gaël Guennebaud, et al. Eigen library for linear algebra. eigen.tuxfamily.org.<br>[26] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675–678. ACM, 2014. arxiv.org/pdf/1408.5093.<br>[27] Andrej Karpathy, George Toderici, Sachin Shetty, Tommy Leung, Rahul Sukthankar, and Li FeiFei. Large-scale video classification with convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 1725–1732. IEEE, 2014. research.google.com/pubs/archive/42455.pdf.<br>[28] A Krizhevsky. Cuda-convnet, 2014. code.google.com/p/cuda-convnet/.<br>[29] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014. arxiv.org/abs/1404.5997.<br>[30] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset. <a href="http://www.cs.toronto.edu/˜kriz/cifar.html" target="_blank" rel="noopener">www.cs.toronto.edu/˜kriz/cifar.html</a>.<br>[31] Quoc Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Greg Corrado, Kai Chen, Jeff Dean, and Andrew Ng. Building high-level features using large scale unsupervised learning. In ICML’2012, 2012. Google Research PDF.<br>[32] Yann LeCun, Corinna Cortes, and Christopher JC Burges. The MNIST database of handwritten digits, 1998. yann.lecun.com/exdb/mnist/.<br>[33] Mu Li, Dave Andersen, and Alex Smola. Parameter server. parameterserver.org.<br>[34] Chris J Maddison, Aja Huang, Ilya Sutskever, and David Silver. Move evaluation in Go using deep convolutional neural networks. arXiv preprint arXiv:1412.6564, 2014. arxiv.org/abs/1412.6564.<br>[35] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In International Conference on Learning Representations: Workshops Track, 2013. arxiv.org/abs/1301.3781.<br>[36] Derek G Murray, Frank McSherry, Rebecca Isaacs, Michael Isard, Paul Barham, and Mart´ ın Abadi. Naiad: a timely dataflow system. In Proceedings of the TwentyFourth ACM Symposium on Operating Systems Principles, pages 439–455. ACM, 2013. Microsoft Research PDF.<br>[37] Derek G. Murray, Malte Schwarzkopf, Christopher Smowton, Steven Smit, Anil Madhavapeddy, and Steven Hand. Ciel: a universal execution engine for distributed data-flow computing. In Proceedings of the Ninth USENIX Symposium on Networked Systems Design and Implementation, 2011. Usenix PDF.<br>[38] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, et al. Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296, 2015. arxiv.org/abs/1507.04296.<br>[39] CUDA Nvidia. Cublas library. NVIDIA Corporation, Santa Clara, California, 15, 2008. developer.nvidia.com/cublas.<br>[40] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. ACM SIGPLAN Notices, 48(6):519–530, 2013. people.csail.mit.edu/fredo/tmp/Halide-5min.pdf.<br>[41] Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, and Vijay Pande. Massively multitask networks for drug discovery. arXiv preprint arXiv:1502.02072, 2015. arxiv.org/abs/1502.02072.<br>[42] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693–701, 2011. papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.<br>[43] Chuck Rosenberg. Improving Photo Search: A step across the semantic gap, 2013. googleresearch.blogspot.com/2013/06/improving-photo-search-step-across.html.<br>[44] Christopher J Rossbach, Yuan Yu, Jon Currey, Jean Philippe Martin, and Dennis Fetterly. Dandelion: a compiler and runtime for heterogeneous systems. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, pages 49–68. ACM, 2013. research-srv.microsoft.com/pubs/201110/sosp13-dandelion-final.pdf.<br>[45] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Cognitive modeling, 5:3, 1988. <a href="http://www.cs.toronto.edu/" target="_blank" rel="noopener">www.cs.toronto.edu/</a> hinton/absps/naturebp.pdf.<br>[46] Has ¸im Sak, Andrew Senior, Kanishka Rao, Franc ¸oise Beaufays, and Johan Schalkwyk. Google Voice Search: faster and more accurate, 2015. googleresearch.blogspot.com/2015/09/google-voice-search-faster-and-more.html.<br>[47] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014. papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural.<br>[48] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR’2015, 2015. arxiv.org/abs/1409.4842.<br>[49] Seiya Tokui. Chainer: A powerful, flexible and intuitive framework of neural networks. chainer.org.<br>[50] Vincent Vanhoucke. Speech recognition and deep learning, 2015. googleresearch.blogspot.com/2012/08/speech-recognition-and-deep-learning.html.<br>[51] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Oppenheimer, Eric Tune, and John Wilkes. Large-scale cluster management at Google with Borg. In Proceedings of the Tenth European Conference on Computer Systems, page 18. ACM, 2015. research.google.com/pubs/archive/43438.pdf.<br>[52] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign language. Technical report, arXiv:1412.7449, 2014. arxiv.org/abs/1412.7449.<br>[53] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In NIPS, 2015. arxiv.org/abs/1506.03134.<br>[54] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, et al. An introduction to computational networks and the computational network toolkit. Technical report, Tech.Rep. MSR, Microsoft Research, 2014, 2014. research.microsoft.com/apps/pubs/?id=226641.<br>[55] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J Franklin, Scott Shenker, and Ion Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012. <a href="http://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf" target="_blank" rel="noopener">www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf</a>.<br>[56] Matthew D. Zeiler, Marc’Aurelio Ranzato, Rajat Monga, Mark Mao, Ke Yang, Quoc Le, Patrick Nguyen, Andrew Senior, Vincent Vanhoucke, Jeff Dean, and Geoffrey E. Hinton. On rectified linear units for speech processing. In ICASSP, 2013. research.google.com/pubs/archive/40811.pdf.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/TensorFlow/" rel="tag"><iclass="fa fa-tag"></i> TensorFlow</a>
          
            <a href="/tags/机器学习/" rel="tag"><iclass="fa fa-tag"></i> 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/04/Docker/" rel="next" title="Docker学习笔记">
                <i class="fa fa-chevron-left"></i> Docker学习笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNjA1OC8xMjU5Mw=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/saber.jpg"
                alt="Twist Nihility" />
            
              <p class="site-author-name" itemprop="name">Twist Nihility</p>
              <p class="site-description motion-element" itemprop="description">是夫喋喋，炫玉而贾石者也。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/TwistNihility" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:2544832237@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://stackoverflow.com/users/6649702/mars-pu?tab=profile" target="_blank" title="Stack OverFlow">
                      
                        <i class="fa fa-fw fa-stack-overflow"></i>Stack OverFlow</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/marspu" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-contao"></i>CSDN</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/pu-jun-81/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-pen-square"></i>知乎</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://juejin.im/user/5ad760c0f265da50574473ee" target="_blank" title="掘金">
                      
                        <i class="fa fa-fw fa-spinner"></i>掘金</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://arxiv.org" title="Arxiv" target="_blank">Arxiv</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://opencv.org" title="OpenCV" target="_blank">OpenCV</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://codepen.io" title="CodePen" target="_blank">CodePen</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://tensorflow.google.cn" title="TensorFlow" target="_blank">TensorFlow</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://developer.mozilla.org/zh-CN" title="Mozilla Developer Networks" target="_blank">Mozilla Developer Networks</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-引言"><span class="nav-number">2.</span> <span class="nav-text">1 引言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-编程模型和基本概念"><span class="nav-number">3.</span> <span class="nav-text">2 编程模型和基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#操作-Operations-和核-Kernels"><span class="nav-number">3.0.1.</span> <span class="nav-text">操作(Operations)和核(Kernels)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#会话-Sessions"><span class="nav-number">3.0.2.</span> <span class="nav-text">会话(Sessions)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量-Variables"><span class="nav-number">3.0.3.</span> <span class="nav-text">变量(Variables)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-实现"><span class="nav-number">4.</span> <span class="nav-text">3 实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#设备-Devices"><span class="nav-number">4.0.1.</span> <span class="nav-text">设备(Devices)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量-Tensors"><span class="nav-number">4.0.2.</span> <span class="nav-text">张量(Tensors)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-单设备执行"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 单设备执行</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-多设备执行"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 多设备执行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-节点放置"><span class="nav-number">4.2.1.</span> <span class="nav-text">3.2.1 节点放置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-交叉设备通信"><span class="nav-number">4.2.2.</span> <span class="nav-text">3.2.2 交叉设备通信</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-分布式执行"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 分布式执行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#容错"><span class="nav-number">4.3.1.</span> <span class="nav-text">容错</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-扩展"><span class="nav-number">5.</span> <span class="nav-text">4 扩展</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-梯度计算-Gradient-Computation"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 梯度计算(Gradient Computation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-部分执行"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 部分执行</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-设备限制"><span class="nav-number">5.3.</span> <span class="nav-text">4.3 设备限制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-控制流程"><span class="nav-number">5.4.</span> <span class="nav-text">4.4 控制流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-输入操作"><span class="nav-number">5.5.</span> <span class="nav-text">4.5 输入操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-队列"><span class="nav-number">5.6.</span> <span class="nav-text">4.6 队列</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-7-容器"><span class="nav-number">5.7.</span> <span class="nav-text">4.7 容器</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-优化"><span class="nav-number">6.</span> <span class="nav-text">5 优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-共同子表达式消除"><span class="nav-number">6.1.</span> <span class="nav-text">5.1 共同子表达式消除</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-控制数据通信和内存分配"><span class="nav-number">6.2.</span> <span class="nav-text">5.2 控制数据通信和内存分配</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-异步核-kernel"><span class="nav-number">6.3.</span> <span class="nav-text">5.3 异步核(kernel)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-用于核-kernel-实现的优化库"><span class="nav-number">6.4.</span> <span class="nav-text">5.4 用于核(kernel)实现的优化库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-有损压缩"><span class="nav-number">6.5.</span> <span class="nav-text">5.5 有损压缩</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-状态和经验"><span class="nav-number">7.</span> <span class="nav-text">6 状态和经验</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-常用编程规范"><span class="nav-number">8.</span> <span class="nav-text">7 常用编程规范</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据并行训练"><span class="nav-number">8.1.</span> <span class="nav-text">数据并行训练</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-性能"><span class="nav-number">9.</span> <span class="nav-text">8 性能</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#9-工具"><span class="nav-number">10.</span> <span class="nav-text">9 工具</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#9-1-TensorBoard：图结构和总结统计的可视化"><span class="nav-number">10.1.</span> <span class="nav-text">9.1 TensorBoard：图结构和总结统计的可视化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算图的可视化"><span class="nav-number">10.2.</span> <span class="nav-text">计算图的可视化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#汇总数据的可视化"><span class="nav-number">10.3.</span> <span class="nav-text">汇总数据的可视化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-2-性能跟踪"><span class="nav-number">10.4.</span> <span class="nav-text">9.2 性能跟踪</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#10-未来工作"><span class="nav-number">11.</span> <span class="nav-text">10 未来工作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#11-相关工作"><span class="nav-number">12.</span> <span class="nav-text">11 相关工作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#12-结论"><span class="nav-number">13.</span> <span class="nav-text">12 结论</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#致谢"><span class="nav-number">14.</span> <span class="nav-text">致谢</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-number">15.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Twist Nihility</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">全站总字数&#58;</span>
    
    <span title="全站总字数">65.6k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("wXTw0k8elDqBTnY5iHd52vlV-gzGzoHsz", "UYebvR19cj60NeNvmfEbbJij");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  


  
   <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
   <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
   <script type="text/javascript" src="/js/src/fireworks.js"></script>


</body>
</html>
