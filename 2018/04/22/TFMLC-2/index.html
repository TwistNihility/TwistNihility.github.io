<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-corner-indicator.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,深度学习,TensorFlow,Python," />





  <link rel="alternate" href="/atom.xml" title="Sisyphus's Utopia" type="application/atom+xml" />






<meta name="description" content="计算图中的操作1234567891011121314151617#导入TensorFlow，创建一个会话，开始一个计算图import tensorflow as tfsess=tf.Session()#传入一个列表到计算图中的操作，并打印返回值import numpy as npx_vals=np.array([1., 3., 5., 7., 9.])">
<meta name="keywords" content="机器学习,深度学习,TensorFlow,Python">
<meta property="og:type" content="article">
<meta property="og:title" content="TFMLC学习笔记（2） TensorFlow进阶">
<meta property="og:url" content="http://yoursite.com/2018/04/22/TFMLC-2/index.html">
<meta property="og:site_name" content="Sisyphus&#39;s Utopia">
<meta property="og:description" content="计算图中的操作1234567891011121314151617#导入TensorFlow，创建一个会话，开始一个计算图import tensorflow as tfsess=tf.Session()#传入一个列表到计算图中的操作，并打印返回值import numpy as npx_vals=np.array([1., 3., 5., 7., 9.])">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-04-23T14:34:01.079Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TFMLC学习笔记（2） TensorFlow进阶">
<meta name="twitter:description" content="计算图中的操作1234567891011121314151617#导入TensorFlow，创建一个会话，开始一个计算图import tensorflow as tfsess=tf.Session()#传入一个列表到计算图中的操作，并打印返回值import numpy as npx_vals=np.array([1., 3., 5., 7., 9.])">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/04/22/TFMLC-2/"/>





  <title>TFMLC学习笔记（2） TensorFlow进阶 | Sisyphus's Utopia</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sisyphus's Utopia</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/22/TFMLC-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Twist Nihility">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/saber.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sisyphus's Utopia">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TFMLC学习笔记（2） TensorFlow进阶</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-22T18:50:03+08:00">
                2018-04-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/04/22/TFMLC-2/" class="leancloud_visitors" data-flag-title="TFMLC学习笔记（2） TensorFlow进阶">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4,898
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  21
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="计算图中的操作"><a href="#计算图中的操作" class="headerlink" title="计算图中的操作"></a>计算图中的操作</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入TensorFlow，创建一个会话，开始一个计算图</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess=tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment">#传入一个列表到计算图中的操作，并打印返回值</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x_vals=np.array([<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">5.</span>, <span class="number">7.</span>, <span class="number">9.</span>])                            <span class="comment">#创建一个numpy数组</span></span><br><span class="line">x_data=tf.palceholder(tf.float32)                                <span class="comment">#声明占位符</span></span><br><span class="line">m_const=tf.constant(<span class="number">3.</span>)                                          <span class="comment">#声明张量</span></span><br><span class="line">my_product=tf.mul(x_data,m_const)</span><br><span class="line"><span class="keyword">for</span> x_val <span class="keyword">in</span> x_vals:</span><br><span class="line">    print(sess.run(my_product,feed_dict=&#123;x_data:x_val&#125;))</span><br><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">9.0</span></span><br><span class="line"><span class="number">15.0</span></span><br><span class="line"><span class="number">21.0</span></span><br><span class="line"><span class="number">27.0</span></span><br></pre></td></tr></table></figure>
<h1 id="TensorFlow的嵌入Layer"><a href="#TensorFlow的嵌入Layer" class="headerlink" title="TensorFlow的嵌入Layer"></a>TensorFlow的嵌入Layer</h1><p>本节学习如何在同一个计算图中进行多个乘法操作。下面将用两个矩阵乘以占位符，然后做加法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入TensorFlow，创建一个会话，开始一个计算图</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess=tf.Session()</span><br></pre></td></tr></table></figure></p>
<p>我们将传入两个形状为3x5的numpy数组，然后每个矩阵乘以常量矩阵（形状：5x1），将返回一个形状为3x1的矩阵。然后再乘以1x1的矩阵，返回的结果矩阵仍然为3x1。最后，加上一个3x1的矩阵。</p>
<ol>
<li><p>首先，创建数据和占位符</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_array=np.array([[ <span class="number">1.</span>, <span class="number">3.</span>,<span class="number">5.</span>,<span class="number">7.</span>,<span class="number">9.</span>],</span><br><span class="line">                   [<span class="number">-2.</span>, <span class="number">0.</span>,<span class="number">2.</span>,<span class="number">4.</span>,<span class="number">6.</span>],</span><br><span class="line">                   [<span class="number">-6.</span>,<span class="number">-3.</span>,<span class="number">0.</span>,<span class="number">3.</span>,<span class="number">6.</span>]])</span><br><span class="line">x_vals=np.array([my_array,my_array+<span class="number">1</span>])</span><br><span class="line">x_data=tf.placeholder(tf.float32,shape=(<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>接着，创建矩阵乘法和加法中要用到的常量矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m1=tf.constant([[<span class="number">1.</span>],[<span class="number">0.</span>],[<span class="number">-1.</span>],[<span class="number">2.</span>],[<span class="number">4.</span>]])</span><br><span class="line">m2=tf.constant([[<span class="number">2.</span>]])</span><br><span class="line">a1=tf.constant([[<span class="number">10.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>声明操作，表示成计算图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prod1=tf.matmul(x_data,m1)</span><br><span class="line">prod2=tf.matmul(prod1,m2)</span><br><span class="line">add1=tf.add(prod2,a1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后，通过计算图赋值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x_val <span class="keyword">in</span> x_vals:</span><br><span class="line">    print(sess.run(add1,feed_dict=&#123;x_data:x_val&#125;))</span><br><span class="line">[[<span class="number">102.</span>]</span><br><span class="line"> [ <span class="number">66.</span>]</span><br><span class="line"> [ <span class="number">58.</span>]]</span><br><span class="line">[[<span class="number">114.</span>]</span><br><span class="line"> [ <span class="number">78.</span>]</span><br><span class="line"> [ <span class="number">70.</span>]]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="TensorFlow的多层Layer"><a href="#TensorFlow的多层Layer" class="headerlink" title="TensorFlow的多层Layer"></a>TensorFlow的多层Layer</h1><p>本节中，将介绍如何更好地连接多层Layer，包括自定义Layer。这里给出一个例子（数据是生成随机图片数据），以更好地理解不同类型的操作和如何利用内建层Layer进行计算。我们对2D图像进行滑动窗口平均，然后通过自定义操作层Layer返回结果。<br>在本节，TensorFlow的计算图太大，导致无法完整查看。为了解决此问题，将对各层Layer和操作进行层级命名管理。<br>按照惯例，加载numpy和TensorFlow模块，创建计算图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">sess=tf.Session()</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p>首先通过numpy创建2D图像，4x4像素图片。我们将创建成四维：第一维和最后一维大小为1。注意TensorFlow的图像函数是处理四维图片的，这四维是：图片数量、高度、宽度、颜色通道。这里是一张图片，单颜色通道，所以设两个维度值为1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_shape=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line">x_val=np.random.uniform(size=x_shape)</span><br></pre></td></tr></table></figure>
</li>
<li><p>下面在计算图中创建占位符。此例中占位符是用来传入图片的，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_data=tf.placeholder(tf.float32,shape=x_shape)</span><br></pre></td></tr></table></figure>
</li>
<li><p>为了创建过滤4x4像素图片的滑动窗口，我们将用TensorFlow内建函数conv2d()（常用来做图像处理）卷积2x2形状的常量窗口。con2d()函数传入滑动窗口、过滤器和步长。本例将在滑动窗口四个方向上计算，所以在四个方向上都要指定步长。创建一个2x2的窗口，每个方向长度为2的步长。为了计算平均值，用常量为0.25的向量与2x2的窗口卷积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_filter=tf.constant(<span class="number">0.25</span>,shape=[<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">my_strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>]</span><br><span class="line">mov_avg_layer=tf.nn.conv2d(x_data,my_filter,my_strides,padding=<span class="string">'SAME'</span>,name=<span class="string">'Moving_Avg_Window'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意，我们通过conv2d()函数的name参数，把这层Layer命名为“Moving_Avg_Window”。</p>
</li>
<li><p>现在定义一个自定义Layer，操作滑动窗口平均的2x2的返回值。自定义函数将输入张量乘以一个2x2的矩阵张量，然后每个元素加1.因为矩阵乘法只计算二维矩阵，所以剪裁图像的多余维度（大小为1）。TensorFlow通过内建函数squeeze()剪裁。下面是新定义的Layer：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a custom layer which will be sigmoid(Ax+b) where x is a 2x2 matrix and A and b are 2x2 matrices</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_layer</span><span class="params">(input_matrix)</span>:</span></span><br><span class="line">    input_matrix_sqeezed = tf.squeeze(input_matrix)</span><br><span class="line">    A = tf.constant([[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">-1.</span>, <span class="number">3.</span>]])</span><br><span class="line">    b = tf.constant(<span class="number">1.</span>, shape=[<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">    temp1 = tf.matmul(A, input_matrix_sqeezed)</span><br><span class="line">    temp = tf.add(temp1, b) <span class="comment"># Ax + b</span></span><br><span class="line">    <span class="keyword">return</span>(tf.sigmoid(temp))</span><br></pre></td></tr></table></figure>
</li>
<li><p>现在把刚刚新定义的Layer加入到计算图中，并且用tf.name_scope()命名唯一的Layer名字，后续在计算图中可折叠/扩展Custom_Layer层。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add custom layer to graph</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Custom_Layer'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    custom_layer1 = custom_layer(mov_avg_layer)</span><br></pre></td></tr></table></figure>
</li>
<li><p>为占位符传入4x4像素图片，然后执行计算图，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># After custom operation, size is now 2x2 (squeezed out size 1 dims)</span></span><br><span class="line">print(sess.run(custom_layer1, feed_dict=&#123;x_data: x_val&#125;))</span><br><span class="line">[[<span class="number">0.91914582</span> <span class="number">0.96025133</span>]</span><br><span class="line"> [<span class="number">0.87262219</span> <span class="number">0.9469803</span>]]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="TensorFlow实现损失函数"><a href="#TensorFlow实现损失函数" class="headerlink" title="TensorFlow实现损失函数"></a>TensorFlow实现损失函数</h1><p>为了优化机器学习学习算法，我们需要评估机器学习模型训练输出结果。在TensorFlow中评估输出结果依赖损失函数。损失函数告诉TensorFlow，预测结果相比期望的结果是好是坏。在大部分场景下，我们会有算法模型训练的样本数据集和目标值。损失函数比较预测值与目标值，并给出两者之间的数值化的差值。<br>为了比较不同损失函数的区别，需要用图表将它们绘制出来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt      <span class="comment">#加载matplotlib（Python的绘图库）</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure></p>
<ol>
<li>回归算法的损失函数。回归算法是预测连续因变量的。创建预测序列和目标序列作为张量，预测序列是-1到1之间的等差数列。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Numerical Predictions</span></span><br><span class="line">x_vals = tf.linspace(<span class="number">-1.</span>, <span class="number">1.</span>, <span class="number">500</span>)</span><br><span class="line">target = tf.constant(<span class="number">0.</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li>L2正则损失函数（欧拉损失函数）<br>L2正则损失函数是预测值与目标值差值的平方和。注意：上述例子中目标值为0。L2正则损失函数是非常有用的损失函数，因为它在目标值附近有更好的曲度，机器学习算法利用这点收敛，并且离目标越近收敛越慢。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L2 loss</span></span><br><span class="line"><span class="comment"># L = (pred - actual)^2</span></span><br><span class="line">l2_y_vals=tf.square(target-x_vals)</span><br><span class="line">l2_y_out=sess.run(l2_y_vals)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>TensorFlow有内建的L2正则形式，称为nn.l2_loss()。这个函数是实际L2正则的一半，即，上述l2_y_vals的1/2</p>
<ul>
<li>L1正则损失函数（绝对值损失函数）<br>与L2正则损失函数对差值求平方不同的是，L1正则损失函数对差值求绝对值。L1正则在目标值附近不平滑，这会导致算法不能很好的收敛。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L1 loss</span></span><br><span class="line"><span class="comment"># L = abs(pred - actual)</span></span><br><span class="line">l1_y_vals = tf.abs(target - x_vals)</span><br><span class="line">l1_y_out = sess.run(l1_y_vals)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>Pseudo-Huber损失函数<br>Pseudo-Huber损失函数是Huber损失函数的连续、平滑估计，试图利用L1和L2正则削减极值处的陡峭，使得目标值附近连续。它的表达式依赖参数delta。我们将绘图来显示delta1=0.25和delta2=5的区别：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pseudo-Huber loss</span></span><br><span class="line"><span class="comment"># L = delta^2 * (sqrt(1 + ((pred - actual)/delta)^2) - 1)</span></span><br><span class="line">delta1 = tf.constant(<span class="number">0.25</span>)</span><br><span class="line">phuber1_y_vals = tf.multiply(tf.square(delta1), tf.sqrt(<span class="number">1.</span> + tf.square((target - x_vals)/delta1)) - <span class="number">1.</span>)</span><br><span class="line">phuber1_y_out = sess.run(phuber1_y_vals)</span><br><span class="line"></span><br><span class="line">delta2 = tf.constant(<span class="number">5.</span>)</span><br><span class="line">phuber2_y_vals = tf.multiply(tf.square(delta2), tf.sqrt(<span class="number">1.</span> + tf.square((target - x_vals)/delta2)) - <span class="number">1.</span>)</span><br><span class="line">phuber2_y_out = sess.run(phuber2_y_vals)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol start="2">
<li>分类损失函数<br>分类损失函数用来评估预测分类结果。<br>重新给x_vals和target赋值，保存返回值并在下节绘制出来。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_vals = tf.linspace(<span class="number">-3.</span>, <span class="number">5.</span>, <span class="number">500</span>)</span><br><span class="line">target = tf.constant(<span class="number">1.</span>)</span><br><span class="line">targets = tf.fill([<span class="number">500</span>,], <span class="number">1.</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li>Hinge损失函数<br>Hinge损失函数主要用来评估支持向量机算法，但有时也用来评估神经网络算法。在本例中是计算两个目标类（-1,1）之间的损失。下面的代码中，使用目标值1，所以预测值离1越近，损失函数值越小：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hinge loss</span></span><br><span class="line"><span class="comment"># Use for predicting binary (-1, 1) classes</span></span><br><span class="line"><span class="comment"># L = max(0, 1 - (pred * actual))</span></span><br><span class="line">hinge_y_vals = tf.maximum(<span class="number">0.</span>, <span class="number">1.</span> - tf.multiply(target, x_vals))</span><br><span class="line">hinge_y_out = sess.run(hinge_y_vals)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>两类交叉熵损失函数（Cross-entropy loss）<br>两类交叉熵损失函数（Cross-entropy loss）有时也作为逻辑损失函数。比如，当预测两类目标0或者1时，希望度量预测值到真实分类值（0或1）的距离，这个距离经常是0到1之间的实数。为了度量这个距离，使用信息论中的交叉熵。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cross entropy loss</span></span><br><span class="line"><span class="comment"># L = -actual * (log(pred)) - (1-actual)(log(1-pred))</span></span><br><span class="line">xentropy_y_vals = - tf.multiply(target, tf.log(x_vals)) - tf.multiply((<span class="number">1.</span> - target), tf.log(<span class="number">1.</span> - x_vals))</span><br><span class="line">xentropy_y_out = sess.run(xentropy_y_vals)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>Sigmoid交叉熵损失函数（Sigmoid cross entropy loss）<br>Sigmoid交叉熵损失函数与两类交叉熵损失函数非常相似，区别在于：Sigmoid交叉熵损失函数先把x_vals值通过sigmoid函数转换，再计算交叉熵损失。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xentropy_sigmoid_y_vals=tf.nn.sigmoid_cross_entropy_with_logits(x_vals,targets)</span><br><span class="line">xentropy_sigmoid_y_out=sess.run(xentropy_sigmoid_y_vals)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>加权交叉熵损失函数（Weighted cross entropy loss）<br>加权交叉熵损失函数是Sigmoid交叉熵损失函数的加权，对正目标加权。例如，将正目标加权权重0.5<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Weighted (softmax) cross entropy loss</span></span><br><span class="line"><span class="comment"># L = -actual * (log(pred)) * weights - (1-actual)(log(1-pred))</span></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="comment"># L = (1 - pred) * actual + (1 + (weights - 1) * pred) * log(1 + exp(-actual))</span></span><br><span class="line">weight = tf.constant(<span class="number">0.5</span>)</span><br><span class="line">xentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(x_vals,targets,weight)</span><br><span class="line">xentropy_weighted_y_out = sess.run(xentropy_weighted_y_vals)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>Softmax交叉熵损失函数（Softmax cross-entropy loss）<br>Softmax交叉熵损失函数是作用于非归一化的输出结果，只针对单个目标分类的计算损失。通过softmax函数将输出结果转化成概率分布，然后计算真值概率分布的损失<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Softmax entropy loss</span></span><br><span class="line"><span class="comment"># L = -actual * (log(softmax(pred))) - (1-actual)(log(1-softmax(pred)))</span></span><br><span class="line">unscaled_logits = tf.constant([[<span class="number">1.</span>, <span class="number">-3.</span>, <span class="number">10.</span>]])</span><br><span class="line">target_dist = tf.constant([[<span class="number">0.1</span>, <span class="number">0.02</span>, <span class="number">0.88</span>]])</span><br><span class="line">softmax_xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(unscaled_logits,target_dist)</span><br><span class="line">print(sess.run(softmax_xentropy))</span><br><span class="line">[<span class="number">1.16012561</span>]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>稀疏Softmax交叉熵损失函数（Sparse softmax cross-entropy loss）<br>稀疏Softmax交叉熵损失函数和Softmax交叉熵损失函数类似，它是把目标分类为true的转化成index，而Softmax交叉熵损失函数将目标转成概率分布。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sparse entropy loss</span></span><br><span class="line"><span class="comment"># Use when classes and targets have to be mutually exclusive</span></span><br><span class="line"><span class="comment"># L = sum( -actual * log(pred) )</span></span><br><span class="line">unscaled_logits = tf.constant([[<span class="number">1.</span>, <span class="number">-3.</span>, <span class="number">10.</span>]])</span><br><span class="line">sparse_target_dist = tf.constant([<span class="number">2</span>])</span><br><span class="line">sparse_xentropy =  tf.nn.sparse_softmax_cross_entropy_with_logits(unscaled_logits,sparse_target_dist)</span><br><span class="line">print(sess.run(sparse_xentropy))</span><br><span class="line">[<span class="number">0.00012564</span>]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>3.绘制各类损失函数</p>
<ul>
<li>用matplotlib绘制回归算法的损失函数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the output:</span></span><br><span class="line">x_array = sess.run(x_vals)</span><br><span class="line">plt.plot(x_array, l2_y_out, <span class="string">'b-'</span>, label=<span class="string">'L2 Loss'</span>)</span><br><span class="line">plt.plot(x_array, l1_y_out, <span class="string">'r--'</span>, label=<span class="string">'L1 Loss'</span>)</span><br><span class="line">plt.plot(x_array, phuber1_y_out, <span class="string">'k-.'</span>, label=<span class="string">'P-Huber Loss (0.25)'</span>)</span><br><span class="line">plt.plot(x_array, phuber2_y_out, <span class="string">'g:'</span>, label=<span class="string">'P-Huber Loss (5.0)'</span>)</span><br><span class="line">plt.ylim(<span class="number">-0.2</span>, <span class="number">0.4</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>, prop=&#123;<span class="string">'size'</span>: <span class="number">11</span>&#125;)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>用matplotlib绘制分类算法的损失函数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the output</span></span><br><span class="line">x_array = sess.run(x_vals)</span><br><span class="line">plt.plot(x_array, hinge_y_out, <span class="string">'b-'</span>, label=<span class="string">'Hinge Loss'</span>)</span><br><span class="line">plt.plot(x_array, xentropy_y_out, <span class="string">'r--'</span>, label=<span class="string">'Cross Entropy Loss'</span>)</span><br><span class="line">plt.plot(x_array, xentropy_sigmoid_y_out, <span class="string">'k-.'</span>, label=<span class="string">'Cross Entropy Sigmoid Loss'</span>)</span><br><span class="line">plt.plot(x_array, xentropy_weighted_y_out, <span class="string">'g:'</span>, label=<span class="string">'Weighted Cross Entropy Loss (x0.5)'</span>)</span><br><span class="line">plt.ylim(<span class="number">-1.5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#plt.xlim(-1, 3)</span></span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>, prop=&#123;<span class="string">'size'</span>: <span class="number">11</span>&#125;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol start="4">
<li><p>总结<br>损失函数            使用类型         优点                       缺点<br>L2                 回归算法         更稳定                     缺少健壮<br>L1                 回归算法         更健壮                     缺少稳定<br>Psuedo-Huber       回归算法         更健壮、稳定                 参数多<br>Hinge              分类算法         常用于SVM的最大距离         异常值导致无边界损失<br>Cross-entropy      分类算法         更稳定                     缺少健壮，出现无边界损失</p>
</li>
<li><p>评价机器学习模型的其他指标<br>模型指标                     描述<br>R平方值<br>RMSE<br>混淆矩阵<br>召回率<br>精准度<br>F值</p>
</li>
</ol>
<h1 id="TensorFlow实现反向传播"><a href="#TensorFlow实现反向传播" class="headerlink" title="TensorFlow实现反向传播"></a>TensorFlow实现反向传播</h1><p>TensorFlow可以维护操作状态和基于反向传播自动地更新模型变量。通过计算图来更新变量，通过最小化损失函数来反向传播误差。实现方法是声明优化函数（optimization function）。一旦声明好优化函数，TensorFlow将通过优化函数在所有的计算图中解决反向传播的项。当传入数据和最小化损失函数，TensorFlow会在计算图中根据状态相应调节变量。</p>
<ol>
<li>回归算法举例<br>从均值为1，标准差为0.1的正态分布中抽样随机数，然后乘以变量A，损失函数为L2正则损失函数。理论上，A的最优值是10，因为生成的样例数据均值是1。</li>
</ol>
<ul>
<li>导入Python的数值计算模块，numpy和tensorflow：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>创建计算图会话:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create graph</span></span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>生成数据，创建占位符和变量A:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create data</span></span><br><span class="line">x_vals = np.random.normal(<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">100</span>)</span><br><span class="line">y_vals = np.repeat(<span class="number">10.</span>, <span class="number">100</span>)</span><br><span class="line">x_data = tf.placeholder(shape=[<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line">y_target = tf.placeholder(shape=[<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create variable (one model parameter = A)</span></span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>增加乘法操作：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add operation to graph</span></span><br><span class="line">my_output = tf.multiply(x_data, A)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>增加L2正则损失函数：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add L2 loss operation to graph</span></span><br><span class="line">loss = tf.square(my_output - y_target)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>在运行之前初始化变量：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize variables</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>声明变量的优化器<br>大部分优化器算法需要知道每步迭代的步长，这距离是由学习率控制的。如果学习率太小，机器学习算法可能耗时很长才能收敛；如果学习率太大，机器学习算法可能会不收敛。相应地导致梯度消失和梯度爆炸等问题。学习率对算法的收敛影响较大。本次使用标准梯度下降法。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create Optimizer</span></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.02</span>)</span><br><span class="line">train_step = my_opt.minimize(loss)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>训练算法<br>迭代101次，并且每25次迭代打印返回结果。选择一个随机的x和y，传入计算图中。TensorFlow将自动地计算损失，调整A偏差来最小化损失：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run Loop</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    rand_index = np.random.choice(<span class="number">100</span>)</span><br><span class="line">    rand_x = [x_vals[rand_index]]</span><br><span class="line">    rand_y = [y_vals[rand_index]]</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>)%<span class="number">25</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step #'</span> + str(i+<span class="number">1</span>) + <span class="string">' A = '</span> + str(sess.run(A)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))</span><br><span class="line"></span><br><span class="line">Here <span class="keyword">is</span> the output:</span><br><span class="line">Step <span class="comment">#25 A=[6.23402166]</span></span><br><span class="line">Loss=<span class="number">16.3173</span></span><br><span class="line">Step <span class="comment">#50 A=[8.50733757]</span></span><br><span class="line">Loss=<span class="number">3.56651</span></span><br><span class="line">Step <span class="comment">#75 A=[9.37753201]</span></span><br><span class="line">Loss=<span class="number">3.03149</span></span><br><span class="line">Step <span class="comment">#100 A=[9.80041122]</span></span><br><span class="line">Loss=<span class="number">0.0990248</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol start="2">
<li>分类算法举例<br>先重置一下之前的TensorFlow计算图，就可以使用相同的TensorFlow脚本继续分类算法的例子。我们试图找到一个优化的转换方法A，它可以把两个正态分布转换到原点，sigmoid函数将正态分布分割成不同的两类。</li>
</ol>
<ul>
<li>重置计算图，并且重新初始化变量:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line">ops.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create graph</span></span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>从正态分布（N(-1,1)，N(3,1)）生成数据。同时也生成目标标签，占位符和偏差变量A：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create data</span></span><br><span class="line">x_vals = np.concatenate((np.random.normal(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">50</span>), np.random.normal(<span class="number">3</span>, <span class="number">1</span>, <span class="number">50</span>)))</span><br><span class="line">y_vals = np.concatenate((np.repeat(<span class="number">0.</span>, <span class="number">50</span>), np.repeat(<span class="number">1.</span>, <span class="number">50</span>)))</span><br><span class="line">x_data = tf.placeholder(shape=[<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line">y_target = tf.placeholder(shape=[<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create variable (one model parameter = A)</span></span><br><span class="line">A = tf.Variable(tf.random_normal(mean=<span class="number">10</span>, shape=[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>增加转换操作<br>这里不必封装sigmoid函数，因为损失函数中会实现此功能：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add operation to graph</span></span><br><span class="line"><span class="comment"># Want to create the operstion sigmoid(x + A)</span></span><br><span class="line"><span class="comment"># Note, the sigmoid() part is in the loss function</span></span><br><span class="line">my_output = tf.add(x_data, A)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>增加维度<br>由于指定的损失函数期望批量数据增加一个批量数的维度，使用expand_dims()函数增加维度：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Now we have to add another dimension to each (batch size of 1)</span></span><br><span class="line">my_output_expanded = tf.expand_dims(my_output, <span class="number">0</span>)</span><br><span class="line">y_target_expanded = tf.expand_dims(y_target, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>初始化变量A：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize variables</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>声明损失函数<br>这里使用一个带非归一化logits的交叉熵的损失函数，同时会用sigmoid函数转换。TensorFlow的nn.sigmoid_cross_entropy_with_logits()函数实现所有这些功能，需要向它传入指定的维度：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add classification loss (cross entropy)</span></span><br><span class="line">xentropy = tf.nn.sigmoid_cross_entropy_with_logits(my_output_expanded, y_target_expanded)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>增加一个优化器函数让TensorFlow知道如何更新和偏差变量：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create Optimizer</span></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(<span class="number">0.05</span>)</span><br><span class="line">train_step = my_opt.minimize(xentropy)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>最后，通过随机选择的数据迭代几百次，相应地更新变量A。每迭代200次打印出损失和变量A的返回值：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run loop</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1400</span>):</span><br><span class="line">    rand_index = np.random.choice(<span class="number">100</span>)</span><br><span class="line">    rand_x = [x_vals[rand_index]]</span><br><span class="line">    rand_y = [y_vals[rand_index]]</span><br><span class="line">    </span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>)%<span class="number">200</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step #'</span> + str(i+<span class="number">1</span>) + <span class="string">' A = '</span> + str(sess.run(A)))</span><br><span class="line">        print(<span class="string">'Loss = '</span> + str(sess.run(xentropy, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))</span><br><span class="line"></span><br><span class="line">Step <span class="comment">#200 A=[3.59597969]</span></span><br><span class="line">Loss=[[<span class="number">0.00126199</span>]]</span><br><span class="line">Step <span class="comment">#400 A=[0.50947344]</span></span><br><span class="line">Loss=[[<span class="number">0.01149425</span>]]</span><br><span class="line">Step <span class="comment">#600 A=[-0.50994617]</span></span><br><span class="line">Loss=[[<span class="number">0.14271219</span>]]</span><br><span class="line">Step <span class="comment">#800 A=[-0.76606178]</span></span><br><span class="line">Loss=[[<span class="number">0.18807337</span>]]</span><br><span class="line">Step <span class="comment">#1000 A=[-0.90859312]</span></span><br><span class="line">Loss=[[<span class="number">0.02346182</span>]]</span><br><span class="line">Step <span class="comment">#1200 A=[-0.86169094]</span></span><br><span class="line">Loss=[[<span class="number">0.05427232</span>]]</span><br><span class="line">Step <span class="comment">#1400 A=[-1.08486211]</span></span><br><span class="line">Loss=[[<span class="number">0.04099189</span>]]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol start="3">
<li>总结</li>
</ol>
<ul>
<li>实现反向传播主要步骤：<ol>
<li>生成数据。</li>
<li>初始化占位符和变量。</li>
<li>创建损失函数。</li>
<li>定义一个优化器算法。</li>
<li>最后，通过随机数据样本进行迭代，更新变量。</li>
</ol>
</li>
</ul>
<ul>
<li>学习率选择方法：<br>学习率       优缺点                 使用场景<br>小学习率     收敛慢，但结果精确      若算法不稳定，先降低学习率<br>大学习率     结果不精确，但收敛快    若算法收敛太慢，可提高学习率<br>有时，标准梯度下降算法会明显卡顿或者收敛变慢，特别是在梯度为0附近的点。为此，TensorFlow的MomentumOptimizer()函数增加了一项势能，前一次迭代过程的梯度下降值的倒数。<br>另一个可以改变的是优化器的步长。理想情况下，对于变化小的变量使用大步长；而变化迅速的变量使用小步长。实现这种优点的常用算法是Adagrad算法。此算法考虑整个历史迭代的变量梯度，TensorFlow中相应功能的实现是AdagradOptimizer()函数。<br>有时，由于Adagrad算法计算整个历史迭代的梯度，导致梯度迅速变为0.解决这个局限性的是Adadelta算法，它限制使用的迭代次数。TensorFlow中相应功能的实现是AdadeltaOptimizer()函数。</li>
</ul>
<h1 id="TensorFlow实现随机训练和批量训练"><a href="#TensorFlow实现随机训练和批量训练" class="headerlink" title="TensorFlow实现随机训练和批量训练"></a>TensorFlow实现随机训练和批量训练</h1><p>扩展之前回归算法的例子——使用随机训练和批量训练</p>
<ol>
<li><p>导入numpy、matplotlib和tensorflow模块，开始一个计算图会话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure>
</li>
<li><p>声明批量大小<br>批量大小是指通过计算图一次传入多少训练数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_size=<span class="number">20</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>声明模型的数据，占位符和变量<br>占位符有两个维度：第一个维度为None，第二个维度是批量训练中的数据量。我们能显示地设置维度为20，也能设为None。必须知道训练模型中的维度，这会阻止不合法的矩阵操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create data</span></span><br><span class="line">x_vals = np.random.normal(<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">100</span>)</span><br><span class="line">y_vals = np.repeat(<span class="number">10.</span>, <span class="number">100</span>)</span><br><span class="line">x_data = tf.placeholder(shape=[<span class="keyword">None</span>,<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line">y_target = tf.placeholder(shape=[<span class="keyword">None</span>,<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create variable (one model parameter = A)</span></span><br><span class="line">A = tf.Variable(tf.random_normal(shape=[<span class="number">1</span>,<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="奥义：复制粘贴调试"><a href="#奥义：复制粘贴调试" class="headerlink" title="奥义：复制粘贴调试"></a>奥义：复制粘贴调试</h1><ol start="4">
<li><p>在计算图中增加矩阵乘法操作<br>矩阵乘法不满足交换律，所以在matmul()函数中的矩阵参数顺序要正确：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add operation to graph</span></span><br><span class="line">my_output = tf.matmul(x_data, A)</span><br></pre></td></tr></table></figure>
</li>
<li><p>改变损失函数<br>批量训练时损失函数是每个数据点L2损失的平均值。在TensorFlow中通过reduce_mean()函数即可实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add L2 loss operation to graph</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(my_output - y_target))</span><br></pre></td></tr></table></figure>
</li>
<li><p>声明优化器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="TensorFlow实现创建分类器"><a href="#TensorFlow实现创建分类器" class="headerlink" title="TensorFlow实现创建分类器"></a>TensorFlow实现创建分类器</h1><h1 id="TensorFlow实现模型评估"><a href="#TensorFlow实现模型评估" class="headerlink" title="TensorFlow实现模型评估"></a>TensorFlow实现模型评估</h1>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"><iclass="fa fa-tag"></i> 机器学习</a>
          
            <a href="/tags/深度学习/" rel="tag"><iclass="fa fa-tag"></i> 深度学习</a>
          
            <a href="/tags/TensorFlow/" rel="tag"><iclass="fa fa-tag"></i> TensorFlow</a>
          
            <a href="/tags/Python/" rel="tag"><iclass="fa fa-tag"></i> Python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/21/front-end-interview-1/" rel="next" title="前端招聘题集（1）">
                <i class="fa fa-chevron-left"></i> 前端招聘题集（1）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/25/the-history-of-javascript/" rel="prev" title="JavaScript： 先天不足的畸形儿">
                JavaScript： 先天不足的畸形儿 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/saber.jpg"
                alt="Twist Nihility" />
            
              <p class="site-author-name" itemprop="name">Twist Nihility</p>
              <p class="site-description motion-element" itemprop="description">是夫喋喋，炫玉而贾石者也。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/TwistNihility" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:2544832237@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://stackoverflow.com/users/6649702/mars-pu?tab=profile" target="_blank" title="Stack OverFlow">
                      
                        <i class="fa fa-fw fa-stack-overflow"></i>Stack OverFlow</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/marspu" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-contao"></i>CSDN</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/pu-jun-81/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-pen-square"></i>知乎</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://juejin.im/user/5ad760c0f265da50574473ee" target="_blank" title="掘金">
                      
                        <i class="fa fa-fw fa-spinner"></i>掘金</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://arxiv.org" title="Arxiv" target="_blank">Arxiv</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://opencv.org" title="OpenCV" target="_blank">OpenCV</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://codepen.io" title="CodePen" target="_blank">CodePen</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://tensorflow.google.cn" title="TensorFlow" target="_blank">TensorFlow</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://developer.mozilla.org/zh-CN" title="Mozilla Developer Networks" target="_blank">Mozilla Developer Networks</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#计算图中的操作"><span class="nav-number">1.</span> <span class="nav-text">计算图中的操作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow的嵌入Layer"><span class="nav-number">2.</span> <span class="nav-text">TensorFlow的嵌入Layer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow的多层Layer"><span class="nav-number">3.</span> <span class="nav-text">TensorFlow的多层Layer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow实现损失函数"><span class="nav-number">4.</span> <span class="nav-text">TensorFlow实现损失函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow实现反向传播"><span class="nav-number">5.</span> <span class="nav-text">TensorFlow实现反向传播</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow实现随机训练和批量训练"><span class="nav-number">6.</span> <span class="nav-text">TensorFlow实现随机训练和批量训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#奥义：复制粘贴调试"><span class="nav-number">7.</span> <span class="nav-text">奥义：复制粘贴调试</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow实现创建分类器"><span class="nav-number">8.</span> <span class="nav-text">TensorFlow实现创建分类器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow实现模型评估"><span class="nav-number">9.</span> <span class="nav-text">TensorFlow实现模型评估</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Twist Nihility</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">全站总字数&#58;</span>
    
    <span title="全站总字数">13.2k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("wXTw0k8elDqBTnY5iHd52vlV-gzGzoHsz", "UYebvR19cj60NeNvmfEbbJij");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  


  
   <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
   <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
   <script type="text/javascript" src="/js/src/fireworks.js"></script>


</body>
</html>
